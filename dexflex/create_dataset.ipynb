{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [INFO] 2024-02-02 17:16:26,232 root:177 - Start loading needed data in memory!\n",
      " [INFO] 2024-02-02 17:16:26,269 data_loader:189 - Mapare file loaded.\n",
      " [INFO] 2024-02-02 17:16:42,710 data_loader:191 - All inflected forms file loaded.\n",
      " [INFO] 2024-02-02 17:16:44,077 data_loader:193 - Mapping word to id and pos file loaded.\n",
      " [INFO] 2024-02-02 17:16:44,973 data_loader:195 - Mapping word id to word and pos file loaded.\n",
      " [INFO] 2024-02-02 17:16:49,479 data_loader:197 - Mapping id to inflected forms file loaded.\n",
      " [INFO] 2024-02-02 17:16:49,881 data_loader:199 - Mapping entry id to lexeme id file loaded.\n",
      " [INFO] 2024-02-02 17:16:52,411 data_loader:201 - Mapping tree id to entry id file loaded.\n",
      " [INFO] 2024-02-02 17:16:52,463 data_loader:203 - Mapping meaning id to tree id file loaded.\n",
      " [INFO] 2024-02-02 17:16:52,626 data_loader:205 - Mapping synonyms file loaded.\n",
      " [INFO] 2024-02-02 17:16:53,095 data_loader:207 - Mapping contexts file loaded.\n",
      " [INFO] 2024-02-02 17:16:53,096 root:209 - The data from jsons files is now loaded in memory!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import json\n",
    "from prototype import find_entryIds, find_treeIds, find_meaningIds\n",
    "from data_loader import load_jsons\n",
    "\n",
    "mapare, all_inflected_forms, word_to_id_pos, id_to_word_pos, id_to_inflected_forms, entry_lexeme, tree_entry, relation, synonyms, context = load_jsons()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prima slaba\n",
    "# # model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# # model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # def calculate_context_similarity(input_sentence, sentences):\n",
    "# #     encoded_sentences = [tokenizer(sentence, return_tensors=\"pt\") for sentence in sentences]\n",
    "# #     input_encoded = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "# #     with torch.no_grad():\n",
    "# #         context_vectors = [model(**sentence)[\"last_hidden_state\"][:, 0, :].squeeze() for sentence in encoded_sentences]\n",
    "# #         input_vector = model(**input_encoded)[\"last_hidden_state\"][:, 0, :].squeeze()\n",
    "\n",
    "# #     similarities = [1 - cosine(input_vector.numpy(), context_vector.numpy()) for context_vector in context_vectors]\n",
    "# #     if len(similarities):\n",
    "# #         return (sum(similarities)/len(similarities))\n",
    "# #     else:\n",
    "# #         return 0\n",
    "# # \n",
    "\n",
    "# # a doua parca mai ok\n",
    "# # model_name = 'bert-base-uncased'\n",
    "# # tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# # model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# # def get_bert_embedding(sentence, model, tokenizer):\n",
    "# #     tokens = tokenizer(sentence, return_tensors='pt')\n",
    "# #     with torch.no_grad():\n",
    "# #         outputs = model(**tokens)\n",
    "# #     return outputs.pooler_output\n",
    "\n",
    "# # def calculate_context_similarity(sentence_test, sentence_train):\n",
    "# #     embedding_sentence_test = get_bert_embedding(sentence_test, model, tokenizer)\n",
    "# #     embedding_sentence_train = get_bert_embedding(sentence_train, model, tokenizer)\n",
    "\n",
    "# #     return 1 - cosine_similarity(embedding_sentence_test, embedding_sentence_train)[0][0]\n",
    "    \n",
    "\n",
    "# # def cos_sim(input_vectors):\n",
    "# #     similarity = cosine_similarity(input_vectors)\n",
    "# #     return similarity\n",
    "\n",
    "# # # get cosine similairty matrix\n",
    "# # def calculate_context_similarity(sentence, sentences_list):\n",
    "# #     module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "# #     # Import the Universal Sentence Encoder's TF Hub module\n",
    "# #     embed = hub.load(module_url)\n",
    "\n",
    "# #     # Reduce logging output.\n",
    "# #     sentence_embedding = embed([sentence])\n",
    "# #     sentences_embeddings = embed(sentences_list)\n",
    "\n",
    "# #     # Calculate cosine similarity between the input sentence and the list of sentences\n",
    "# #     similarity_matrix = cos_sim(np.vstack([sentence_embedding, sentences_embeddings]))\n",
    "\n",
    "# #     # Extract similarity values for the input sentence compared to each sentence in the list\n",
    "# #     similarities = similarity_matrix[0, 1:]\n",
    "\n",
    "# #     max_similarity_index = np.argmax(similarities)\n",
    "\n",
    "# #     # Print the sentence with the maximum similarity\n",
    "# #     max_similarity_sentence = sentences_list[max_similarity_index]\n",
    "# #     max_similarity_value = similarities[max_similarity_index]\n",
    "\n",
    "# #     # print(f\"Maximum similarity between '{sentence}' and '{max_similarity_sentence}': {max_similarity_value}\")\n",
    "\n",
    "# #     return max_similarity_value\n",
    "\n",
    "\n",
    "# train = json.load(open(\"model_testing/train.json\"))\n",
    "# test = json.load(open(\"model_testing/test.json\"))\n",
    "# all_contexts = json.load(open(\"util/context.json\"))\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from scipy.spatial.distance import cosine\n",
    "# import torch\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import json\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",)\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\",output_hidden_states=True)\n",
    "\n",
    "# #create embeddings\n",
    "# def get_embeddings(text,token_length):\n",
    "#   tokens=tokenizer(text,max_length=token_length,padding='max_length',truncation=True)\n",
    "#   output=model(torch.tensor(tokens.input_ids).unsqueeze(0),\n",
    "#                attention_mask=torch.tensor(tokens.attention_mask).unsqueeze(0)).hidden_states[-1]\n",
    "#   return torch.mean(output,axis=1).detach().numpy()\n",
    "\n",
    "# #calculate similarity\n",
    "# def calculate_similarity(input,compared_to,token_length=20):\n",
    "#     out1=get_embeddings(input,token_length=token_length)#create embeddings of text\n",
    "#     out3=get_embeddings(compared_to,token_length=token_length)#create embeddings of text\n",
    "#     sim1= cosine_similarity(out1,out3)[0][0]\n",
    "\n",
    "#     return sim1\n",
    "\n",
    "# test_ctx_max_idx = 0\n",
    "# train_ctx_max_idx = 0\n",
    "\n",
    "# # evaluare provizorie:\n",
    "# # 44555 corect\n",
    "# # 39678 corect\n",
    "# # 6 wrong\n",
    "# # 9 wrong\n",
    "# # 196 wrong\n",
    "# # 255 wrong\n",
    "# # 44597 wrong\n",
    "# # 44644 corect\n",
    "\n",
    "# results = {}\n",
    "# # in contextele de la ambele omonime cauta cea mai mare similaritate si salveaz o (fa tu o propozitie si compar o cu toate)\n",
    "\n",
    "# for id in [\"44555\"]:\n",
    "#     if len(entry_lexeme.entry_lexeme[id]) > 1:\n",
    "#         entry_ids = find_entryIds([id])\n",
    "#         tree_ids = find_treeIds(entry_ids)\n",
    "#         tree_ids_test = find_treeIds(entry_ids)\n",
    "#         tree_ids_train = find_treeIds(entry_ids)\n",
    "\n",
    "#         if len(tree_ids) > 0:\n",
    "#             for treeId_test in tree_ids_test:\n",
    "#                 max_similarity = 0\n",
    "\n",
    "#                 for test_ctx in test[str(treeId_test)]:\n",
    "#                     print(test_ctx)\n",
    "#                     for treeId_train in tree_ids_train:\n",
    "#                         mean_similarity = []\n",
    "#                         sim = 0\n",
    "#                         for train_ctx in train[str(treeId_train)]:\n",
    "                                        \n",
    "#                             actual_similarity = calculate_similarity(input=test_ctx, compared_to=train_ctx)\n",
    "                            \n",
    "#                             # mean_similarity.append(actual_similarity)\n",
    "                            \n",
    "#                             # provizoriu\n",
    "#                             if actual_similarity > sim:\n",
    "#                                 sim = actual_similarity\n",
    "#                             # try:\n",
    "#                             #     media = sum(mean_similarity) / len(mean_similarity)\n",
    "\n",
    "#                             #     if media > max_similarity:\n",
    "#                             #             max_similarity = media\n",
    "#                             #             max_test_ctx = test_ctx\n",
    "#                             #             max_train_ctx = train_ctx\n",
    "#                                 aux = {str(treeId_train): (actual_similarity, test_ctx)}\n",
    "                                        \n",
    "#                             # except ZeroDivisionError:\n",
    "#                             #     print(\"pass\")\n",
    "                    \n",
    "#                 results[str(treeId_test)] = aux\n",
    "                     \n",
    "# for key in results:\n",
    "#     for key2 in results[key]:\n",
    "#         print(key, \": \", key2, \" => \", results[key][key2])\n",
    "# del results\n",
    "# # with open(\"model_testing/result.json\", \"w\") as fisier_json:\n",
    "#         # json.dump(results, fisier_json, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random tree id din care \"fur\" doua exemple din lista de contexte = test data\n",
    "# compar test data cu alte 10 id uri + id ul din care l-am creat si ma astept sa fie match pe id\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
