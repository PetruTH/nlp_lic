{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [INFO] 2024-01-06 17:58:45,990 root:177 - Start loading needed data in memory!\n",
      " [INFO] 2024-01-06 17:58:45,994 data_loader:189 - Mapare file loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [INFO] 2024-01-06 17:58:53,641 data_loader:191 - All inflected forms file loaded.\n",
      " [INFO] 2024-01-06 17:58:54,217 data_loader:193 - Mapping word to id and pos file loaded.\n",
      " [INFO] 2024-01-06 17:58:54,791 data_loader:195 - Mapping word id to word and pos file loaded.\n",
      " [INFO] 2024-01-06 17:58:58,119 data_loader:197 - Mapping id to inflected forms file loaded.\n",
      " [INFO] 2024-01-06 17:59:00,725 data_loader:199 - Mapping entry id to lexeme id file loaded.\n",
      " [INFO] 2024-01-06 17:59:00,922 data_loader:201 - Mapping tree id to entry id file loaded.\n",
      " [INFO] 2024-01-06 17:59:00,973 data_loader:203 - Mapping meaning id to tree id file loaded.\n",
      " [INFO] 2024-01-06 17:59:01,128 data_loader:205 - Mapping synonyms file loaded.\n",
      " [INFO] 2024-01-06 17:59:01,596 data_loader:207 - Mapping contexts file loaded.\n",
      " [INFO] 2024-01-06 17:59:01,597 root:209 - The data from jsons files is now loaded in memory!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import json\n",
    "from prototype import find_entryIds, find_treeIds, find_meaningIds\n",
    "from data_loader import load_jsons\n",
    "\n",
    "mapare, all_inflected_forms, word_to_id_pos, id_to_word_pos, id_to_inflected_forms, entry_lexeme, tree_entry, relation, synonyms, context = load_jsons()\n",
    "\n",
    "contexts_found = {}\n",
    "test = {}\n",
    "train = {}\n",
    "\n",
    "for id in entry_lexeme.entry_lexeme:\n",
    "    if len(entry_lexeme.entry_lexeme[id]) > 1:\n",
    "        entry_ids = find_entryIds([id])\n",
    "        tree_ids = find_treeIds(entry_ids)\n",
    "        meaning_ids = find_meaningIds(tree_ids)\n",
    "\n",
    "        for treeId in tree_ids:\n",
    "            \n",
    "            if len(context.find_context(treeId)) > 1:\n",
    "                test[treeId] = []\n",
    "                train[treeId] = context.find_context(treeId)\n",
    "                rand_test = random.randint(1, math.floor(len(train[treeId]) / 2) + 1)\n",
    "                \n",
    "                for i in range(rand_test):\n",
    "                    rand_index = random.randint(0, len(train[treeId]) - 1)\n",
    "                    test[treeId].append(train[treeId][rand_index])\n",
    "                    train[treeId].pop(rand_index)\n",
    "\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"model_testing\"):\n",
    "    os.makedirs(\"model_testing\")\n",
    "\n",
    "with open(\"model_testing/train.json\", \"w\") as fisier_json:\n",
    "        json.dump(train, fisier_json, indent=4, sort_keys=True)\n",
    "\n",
    "with open(\"model_testing/test.json\", \"w\") as fisier_json:\n",
    "        json.dump(test, fisier_json, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/inttstbrd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/inttstbrd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      " [INFO] 2024-01-06 18:03:31,671 sentence_transformers.SentenceTransformer:66 - Load pretrained SentenceTransformer: xlm-r-100langs-bert-base-nli-stsb-mean-tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [INFO] 2024-01-06 18:03:37,309 sentence_transformers.SentenceTransformer:105 - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specie de uliu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'282925': {'282925': (tensor([[0.7921]]), '@Porumb@ + sufix $-ar.$', 0)}}\n",
      "Arbust sălbatic din familia rozaceelor, cu ramuri spinoase, cu flori albe și cu fructe sferice de culoare neagră-vineție, cu gust acru, astringent; porumbel[125759] $(Prunus spinosa).$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'282925': {'282925': (tensor([[0.7921]]), '@Porumb@ + sufix $-ar.$', 0)}, '44470': {'44470': (tensor([[0.4818]]), '@Porumbă[125746]@ + sufix $-ar.$', 0)}}\n",
      "aici\n",
      "282925 :  282925  =>  (tensor([[0.7921]]), '@Porumb@ + sufix $-ar.$', 0)\n",
      "44470 :  44470  =>  (tensor([[0.4818]]), '@Porumbă[125746]@ + sufix $-ar.$', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = json.load(open(\"model_testing/train.json\"))\n",
    "test = json.load(open(\"model_testing/test.json\"))\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def keep_letters_and_digits(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('romanian'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = SnowballStemmer(\"romanian\")\n",
    "    tokens = [stemmer.stem(word) for word in tokens]    \n",
    "    return tokens\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "def calculate_context_similarity(sentence1, sentence2):\n",
    "    emb1 = model.encode(sentence1)\n",
    "    emb2 = model.encode(sentence2)\n",
    "    cos_sim = util.cos_sim(emb1, emb2)\n",
    "    return cos_sim\n",
    "\n",
    "test_ctx_max_idx = 0\n",
    "train_ctx_max_idx = 0\n",
    "\n",
    "# evaluare provizorie:\n",
    "# 44555 corect\n",
    "# 39678 corect\n",
    "# 6 wrong\n",
    "# 9 wrong\n",
    "# 196 wrong\n",
    "# 255 wrong\n",
    "# 44597 wrong\n",
    "# 44644 corect\n",
    "\n",
    "results = {}\n",
    "\n",
    "for id in [\"44644\"]:\n",
    "    if len(entry_lexeme.entry_lexeme[id]) > 1:\n",
    "        entry_ids = find_entryIds([id])\n",
    "        tree_ids = find_treeIds(entry_ids)\n",
    "        tree_ids_test = find_treeIds(entry_ids)\n",
    "        tree_ids_train = find_treeIds(entry_ids)\n",
    "\n",
    "        if len(tree_ids) > 0:\n",
    "            for treeId_test in tree_ids_test:\n",
    "                max_similarity = 0\n",
    "\n",
    "                for test_ctx in test[str(treeId_test)]:\n",
    "                    print(test_ctx)\n",
    "                    for treeId_train in tree_ids_train:\n",
    "                        mean_similarity = []\n",
    "\n",
    "                        for train_ctx in train[str(treeId_train)]:\n",
    "                                        \n",
    "                            actual_similarity = calculate_context_similarity(\n",
    "                                            \" \".join(keep_letters_and_digits(test_ctx)),\n",
    "                                            \" \".join(keep_letters_and_digits(train_ctx))\n",
    "                                )\n",
    "                            mean_similarity.append(actual_similarity)\n",
    "                            \n",
    "                            # provizoriu\n",
    "                            try:\n",
    "                                media = sum(mean_similarity) / len(mean_similarity)\n",
    "\n",
    "                                if media > max_similarity:\n",
    "                                        max_similarity = media\n",
    "                                        max_test_ctx = test_ctx\n",
    "                                        max_train_ctx = train_ctx\n",
    "                                        aux = {str(treeId_train): (max_similarity, max_train_ctx, train_ctx_max_idx)}\n",
    "                                        \n",
    "                            except ZeroDivisionError:\n",
    "                                print(\"pass\")\n",
    "                    \n",
    "                results[str(treeId_test)] = aux\n",
    "                print(results)        \n",
    "                    \n",
    "print(\"aici\")\n",
    " \n",
    "for key in results:\n",
    "    for key2 in results[key]:\n",
    "        print(key, \": \", key2, \" => \", results[key][key2])\n",
    "del results\n",
    "# with open(\"model_testing/result.json\", \"w\") as fisier_json:\n",
    "        # json.dump(results, fisier_json, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prima slaba\n",
    "# model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# def calculate_context_similarity(input_sentence, sentences):\n",
    "#     encoded_sentences = [tokenizer(sentence, return_tensors=\"pt\") for sentence in sentences]\n",
    "#     input_encoded = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         context_vectors = [model(**sentence)[\"last_hidden_state\"][:, 0, :].squeeze() for sentence in encoded_sentences]\n",
    "#         input_vector = model(**input_encoded)[\"last_hidden_state\"][:, 0, :].squeeze()\n",
    "\n",
    "#     similarities = [1 - cosine(input_vector.numpy(), context_vector.numpy()) for context_vector in context_vectors]\n",
    "#     if len(similarities):\n",
    "#         return (sum(similarities)/len(similarities))\n",
    "#     else:\n",
    "#         return 0\n",
    "# \n",
    "\n",
    "# a doua parca mai ok\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# def get_bert_embedding(sentence, model, tokenizer):\n",
    "#     tokens = tokenizer(sentence, return_tensors='pt')\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**tokens)\n",
    "#     return outputs.pooler_output\n",
    "\n",
    "# def calculate_context_similarity(sentence_test, sentence_train):\n",
    "#     embedding_sentence_test = get_bert_embedding(sentence_test, model, tokenizer)\n",
    "#     embedding_sentence_train = get_bert_embedding(sentence_train, model, tokenizer)\n",
    "\n",
    "#     return 1 - cosine_similarity(embedding_sentence_test, embedding_sentence_train)[0][0]\n",
    "    \n",
    "\n",
    "# def cos_sim(input_vectors):\n",
    "#     similarity = cosine_similarity(input_vectors)\n",
    "#     return similarity\n",
    "\n",
    "# # get cosine similairty matrix\n",
    "# def calculate_context_similarity(sentence, sentences_list):\n",
    "#     module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "#     # Import the Universal Sentence Encoder's TF Hub module\n",
    "#     embed = hub.load(module_url)\n",
    "\n",
    "#     # Reduce logging output.\n",
    "#     sentence_embedding = embed([sentence])\n",
    "#     sentences_embeddings = embed(sentences_list)\n",
    "\n",
    "#     # Calculate cosine similarity between the input sentence and the list of sentences\n",
    "#     similarity_matrix = cos_sim(np.vstack([sentence_embedding, sentences_embeddings]))\n",
    "\n",
    "#     # Extract similarity values for the input sentence compared to each sentence in the list\n",
    "#     similarities = similarity_matrix[0, 1:]\n",
    "\n",
    "#     max_similarity_index = np.argmax(similarities)\n",
    "\n",
    "#     # Print the sentence with the maximum similarity\n",
    "#     max_similarity_sentence = sentences_list[max_similarity_index]\n",
    "#     max_similarity_value = similarities[max_similarity_index]\n",
    "\n",
    "#     # print(f\"Maximum similarity between '{sentence}' and '{max_similarity_sentence}': {max_similarity_value}\")\n",
    "\n",
    "#     return max_similarity_value\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
