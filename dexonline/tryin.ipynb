{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gata..\n"
     ]
    }
   ],
   "source": [
    "banned_ent_types = {\"ORGANIZATION\", \"EVENT\", \"GPE\", \"LOC\"}\n",
    "banned_pos = [\"PUNCT\", \"SPACE\"]\n",
    "reflexive_deps = [\"expl:poss\", \"expl:pv\", \"iobj\", \"obj\"]\n",
    "root_forms = [\"ROOT\", \"advcl\", \"acl\", \"cop\", \"conj\", \"parataxis\"]\n",
    "\n",
    "reflexive_short_to_long_form = {\n",
    "    \"mi-\": \"îmi\",\n",
    "    \"ți-\": \"îți\",\n",
    "    \"și-\": \"își\",\n",
    "    \"v-\": \"vă\",\n",
    "    \"s-\": \"se\",\n",
    "    \"ne-\": \"ne\",\n",
    "    \"te-\": \"te\"\n",
    "}\n",
    "\n",
    "ud_to_dex = {\n",
    "        \"VERB\": \"V\",\n",
    "        \"AUX\": \"V\",\n",
    "        \"PART\": \"I\",\n",
    "        \"NOUN\": \"M\",\n",
    "        \"PROPN\": \"SP\",\n",
    "        \"PRON\": \"P\",\n",
    "        \"DET\": \"P\",\n",
    "        \"SCONJ\": \"I\",\n",
    "        \"CCONJ\": \"I\",\n",
    "        \"NUM\": \"P\",\n",
    "        \"INTJ\": \"I\",\n",
    "        \"ADV\": \"I\",\n",
    "        \"ADP\": \"I\",\n",
    "        \"ADJ\": \"A\",\n",
    "        \"X\": \"V\"\n",
    "   }\n",
    "end_of_phrase = [\"!\", \"?\", \".\", \"\\n\"]\n",
    "\n",
    "json_archive = \"utils_json.zip\"\n",
    "json_archive_url = f\"https://github.com/PetruTH/nlp_lic/releases/download/Resources/{json_archive}\"\n",
    "UNIDENTIFIED_TOKEN = \"unidentified\"\n",
    "MAPARE_PATH = \"util/forme_morfologice.json\"\n",
    "ALL_INFLECTED_FORMS_PATH = \"util/inflected_form_lexemeId_inflectionId.json\"\n",
    "WORD_TO_ID_POS_PATH = \"util/word_id_pos.json\"\n",
    "ID_TO_WORD_POS_PATH = \"util/id_word_pos.json\"\n",
    "ID_TO_INFLECTED_FORMS_PATH = \"util/wordId_inflected_forms.json\"\n",
    "print(\"gata..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [INFO] 2024-04-08 21:38:14,358 root:177 - Start loading needed data in memory!\n",
      " [INFO] 2024-04-08 21:38:14,361 data_loader:189 - Mapare file loaded.\n",
      " [INFO] 2024-04-08 21:38:20,098 data_loader:191 - All inflected forms file loaded.\n",
      " [INFO] 2024-04-08 21:38:21,186 data_loader:193 - Mapping word to id and pos file loaded.\n",
      " [INFO] 2024-04-08 21:38:22,303 data_loader:195 - Mapping word id to word and pos file loaded.\n",
      " [INFO] 2024-04-08 21:38:25,506 data_loader:197 - Mapping id to inflected forms file loaded.\n",
      " [INFO] 2024-04-08 21:38:26,660 data_loader:199 - Mapping entry id to lexeme id file loaded.\n",
      " [INFO] 2024-04-08 21:38:26,849 data_loader:201 - Mapping tree id to entry id file loaded.\n",
      " [INFO] 2024-04-08 21:38:26,898 data_loader:203 - Mapping meaning id to tree id file loaded.\n",
      " [INFO] 2024-04-08 21:38:27,052 data_loader:205 - Mapping synonyms file loaded.\n",
      " [INFO] 2024-04-08 21:38:28,389 data_loader:207 - Mapping contexts file loaded.\n",
      " [INFO] 2024-04-08 21:38:28,390 root:209 - The data from jsons files is now loaded in memory!\n"
     ]
    }
   ],
   "source": [
    "from data_loader import load_jsons\n",
    "from util_data import (\n",
    "    UNIDENTIFIED_TOKEN,\n",
    "    ud_to_dex,\n",
    "    banned_pos,\n",
    "    banned_ent_types,\n",
    "    reflexive_deps,\n",
    "    reflexive_short_to_long_form\n",
    ")\n",
    "from spacy.tokens import Token\n",
    "\n",
    "mapare, all_inflected_forms, word_to_id_pos, id_to_word_pos, id_to_inflected_forms, entry_lexeme, tree_entry, relation, synonyms, context = load_jsons()\n",
    "\n",
    "\n",
    "def get_all_forms_worker(token: Token) -> [int]:\n",
    "    \"\"\"\n",
    "    thiw will extract every word having inflected form == token.text\n",
    "    \"\"\"\n",
    "    token_text = token.text\n",
    "    if \"-\" in token.text:\n",
    "        token_text = token_text.replace(\"-\", \"\")\n",
    "\n",
    "    all_inflected_words_found = all_inflected_forms.find_all_inflected_forms_double_verification(\n",
    "                token_text, token_text.lower()\n",
    "            )\n",
    "\n",
    "    if all_inflected_words_found == UNIDENTIFIED_TOKEN:\n",
    "        return []\n",
    "\n",
    "    words_prel = []\n",
    "    only_one_word = [word['lexemeId'] for word in all_inflected_words_found]\n",
    "\n",
    "    if len(set(only_one_word)) == 1:\n",
    "        words_prel.append(str(only_one_word[0]))\n",
    "    for word in all_inflected_words_found:\n",
    "        pos_found = mapare.find_dexonline_pos_id(word['inflectionId'])\n",
    "        \"\"\"\n",
    "            mapare.mapping['DEXONLINE_MORPH']: [\"morph dexonline\", \"pos dexonline\"],\n",
    "            this will help for mapping spacy pos to dexonline pos\n",
    "            mapping spacy pos with dexonline pos\n",
    "            looking after an id found from dexonline\n",
    "        \"\"\"\n",
    "\n",
    "        if ud_to_dex[token.pos_] == pos_found:\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "        elif ud_to_dex[token.pos_] == \"M\" and pos_found == \"F\":\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "        elif ud_to_dex[token.pos_] == \"M\" and pos_found == \"N\":\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "    words_prel.sort(key=lambda x: int(x))\n",
    "\n",
    "    return words_prel\n",
    "\n",
    "\n",
    "def get_all_forms(token: Token) -> [{str, str}]:\n",
    "    \"\"\"\n",
    "        This function will return all the inflected forms for a certain token given as a parameter.\n",
    "        It will search for that token in dexonline database and it will find the lexemeId.\n",
    "        Based on get_all_forms_worker, it will choose the word from the list returned that\n",
    "        has lemma like the first form found in dexonline database. After, that,\n",
    "        based on that lexemeId, it will return all inflected forms found with the same lexemeId (a list of\n",
    "        dictionaries containig words form and morphological details also from dexonline database)\n",
    "    \"\"\"\n",
    "    words_prel = get_all_forms_worker(token)\n",
    "    token_text = token.text\n",
    "\n",
    "    if len(words_prel) > 1:\n",
    "        for element in words_prel:\n",
    "            if id_to_word_pos.find_id_to_word_pos_form(element) == token.lemma_:\n",
    "                id = element\n",
    "\n",
    "    elif len(words_prel) == 1:\n",
    "        id = words_prel[0]\n",
    "\n",
    "    elif len(words_prel) == 0:\n",
    "        words_found = word_to_id_pos.find_word_id_pos_double_verification(token.lemma_, token_text)\n",
    "\n",
    "        if words_found != UNIDENTIFIED_TOKEN:\n",
    "            words_prel = [str(x['id']) for x in words_found]\n",
    "            id = words_prel[0]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    result = id_to_inflected_forms.find_id_to_inflected_forms(id)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def validate_token(token: Token) -> bool:\n",
    "    \"\"\"\n",
    "        Function that validates if a token can be found in dexonline database.\n",
    "        It will exclude words that describe names or places, organizations, etc.\n",
    "    \"\"\"\n",
    "    if \"-\" in token.text:\n",
    "        return True\n",
    "    if token.pos_ in banned_pos:\n",
    "        return False\n",
    "    if token.lang_ != \"ro\":\n",
    "        return False\n",
    "    if not token.text.isalpha():\n",
    "        return False\n",
    "    if token.ent_type_ in banned_ent_types:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_wanted_form(token: Token, pos_finder: str, person: str, number: str) -> str:\n",
    "    \"\"\"\n",
    "       This function will return the morph form wanted by pos_finder, person and number\n",
    "    \"\"\"\n",
    "    all_morph = get_all_forms(token)\n",
    "    for wanted_form in all_morph:\n",
    "        if pos_finder in wanted_form['pos'] and person in wanted_form['pos'] and number in wanted_form['pos']:\n",
    "            return wanted_form['form']\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def verify_word_at_certain_pos(token: Token, pos_verifier: str) -> bool:\n",
    "    \"\"\"\n",
    "    verifiy if a token is contains a specified string in its part of speech\n",
    "    for example this function will return true if a verb has this description from dexonline\n",
    "    as its pos \"Verb, Indicativ, perfect simplu, persoana I, singular\" and pos_verifier parameter\n",
    "    is \"perfect simplu\" or \"persoana I\", etc\n",
    "    \"\"\"\n",
    "    all_morph = get_all_forms(token)\n",
    "    for wanted_form in all_morph:\n",
    "        if token.text == wanted_form['form']:\n",
    "            for pos in pos_verifier:\n",
    "                if pos in wanted_form['pos']:\n",
    "                    return True\n",
    "\n",
    "\n",
    "def is_composed_subj(token: Token) -> bool:\n",
    "    # extra step to verify if there is a composed subject (like 'eu cu tine mergem')\n",
    "    if not token.pos_ == \"VERB\" and not token.pos_ == \"AUX\":\n",
    "        if len(list(token.children)):\n",
    "            for t in token.children:\n",
    "                if t.text not in [\"m\", \"te\", \"s\"]:\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_right_person_and_number(token: Token) -> (str, str):\n",
    "    \"\"\"\n",
    "        This function will get the person and number data from token.morph\n",
    "        and will convert these into dexonline database format information\n",
    "        in order to select right form of verb.\n",
    "    \"\"\"\n",
    "    # extract correct person and number for a phrase\n",
    "    person = token.morph.get(\"Person\", ['3'])\n",
    "    number = token.morph.get(\"Number\", ['Sing'])\n",
    "\n",
    "    if is_composed_subj(token):\n",
    "        number = [\"Plur\"]\n",
    "\n",
    "    # formatting number and person to be recognized dexonline json\n",
    "    actual_number = \"plural\" if number == [\"Plur\"] else \"singular\"\n",
    "\n",
    "    if person == ['1']:\n",
    "        actual_person = \"I\"\n",
    "    elif person == ['2']:\n",
    "        actual_person = \"II\"\n",
    "    elif person == ['3']:\n",
    "        actual_person = \"III\"\n",
    "\n",
    "    return actual_number, actual_person\n",
    "\n",
    "\n",
    "def forme_reflexive_verifier(token: Token) -> str:\n",
    "    \"\"\"\n",
    "        This function will map short reflexive forms into long ones\n",
    "        using data from reflexive_deps from util_data.py\n",
    "    \"\"\"\n",
    "    word_added = token.text\n",
    "    if token.dep_ in reflexive_deps:\n",
    "        case_condition = token.morph.get(\"Case\", [\"dummy\"])[0] in [\"Dat\", \"Acc\"]\n",
    "        variant_condition = token.morph.get(\"Variant\", [\"dummy\"])[0] == \"Short\"\n",
    "        if case_condition and variant_condition:\n",
    "            word_added = reflexive_short_to_long_form[token.text]\n",
    "\n",
    "    return word_added\n",
    "\n",
    "\n",
    "\n",
    "from spacy.tokens import Token\n",
    "\n",
    "Token.set_extension(\"forms_\", method=get_all_forms, force=True)\n",
    "Token.set_extension(\"is_valid\", method=validate_token, force=True)\n",
    "\n",
    "\n",
    "def find_lexeme_ids(inflected_forms: [str]) -> [str]:\n",
    "    possible_lexeme_ids = []\n",
    "\n",
    "    if inflected_forms != [\"UNKNOWN\"]:\n",
    "        for inflected_form in inflected_forms:\n",
    "            if inflected_form.get(\"lexemeId\") not in possible_lexeme_ids:\n",
    "                possible_lexeme_ids.append(inflected_form.get(\"lexemeId\"))\n",
    "  \n",
    "    \n",
    "    return possible_lexeme_ids\n",
    "\n",
    "def find_inflection_possibilites(token: Token, inflected_forms: [str], pos_wanted: str) -> [str]:\n",
    "    inflection_possibilites = []\n",
    "\n",
    "    if inflected_forms != [\"UNKNOWN\"]:\n",
    "        for inflected_form in inflected_forms:\n",
    "            inflectionId = mapare.find_dexonline_pos_id(inflected_form[\"inflectionId\"])\n",
    "            \n",
    "            inflected_form_id = str(inflected_form[\"inflectionId\"])\n",
    "\n",
    "            if inflectionId == pos_wanted and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif inflectionId in [\"VT\", \"V\"] and pos_wanted in [\"V\", \"VT\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif inflectionId in [\"M\", \"F\", \"N\"] and pos_wanted in [\"M\", \"F\", \"N\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif token.dep_ in [\"ROOT\", \"nmod\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "\n",
    "    return inflection_possibilites\n",
    "\n",
    "def find_matching_lexemeIds(possible_lexeme_ids: [str], pos_wanted: str) -> [str]:\n",
    "    lexeme_ids = [] \n",
    "   \n",
    "    for lexemeId in possible_lexeme_ids:\n",
    "        variant = id_to_word_pos.find_id_to_word_pos(lexemeId)\n",
    "        if variant['pos'] == pos_wanted:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        elif variant['pos'] in [\"VT\", \"V\", \"AUX\"] and pos_wanted in [\"V\", \"VT\", \"AUX\"]:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        elif variant['pos'] in [\"M\", \"F\", \"N\"] and pos_wanted in [\"M\", \"F\", \"N\"]:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "    \n",
    "    return lexeme_ids\n",
    "\n",
    "def find_entryIds(lexeme_ids: str) -> str:\n",
    "    entry_ids = []\n",
    "    for lexemeId in lexeme_ids:\n",
    "        all_entries = entry_lexeme.find_entry_lexeme(lexemeId)\n",
    "        if all_entries != [\"no entry\"]:\n",
    "            for entry in all_entries:\n",
    "                entry_ids.append(entry)\n",
    "\n",
    "    return entry_ids\n",
    "\n",
    "def find_treeIds(entry_ids: str) -> str:\n",
    "    tree_ids = []\n",
    "    for entryId in entry_ids:\n",
    "        tree_entries = tree_entry.find_tree_entry(entryId)\n",
    "        if tree_entries != [\"no entry tree\"]:\n",
    "            for treeId in tree_entries:\n",
    "                tree_ids.append(treeId)\n",
    "    \n",
    "    return tree_ids\n",
    "\n",
    "def find_meaningIds(tree_ids: str) -> str:\n",
    "    meaning_ids = []\n",
    "\n",
    "    for treeId in tree_ids:\n",
    "        all_meaningIds = relation.find_relation(str(treeId))\n",
    "        if all_meaningIds != [\"no relation\"]:\n",
    "            for meaningId in all_meaningIds:\n",
    "                meaning_ids.append(meaningId)\n",
    "\n",
    "    return meaning_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inttstbrd/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from spacy.tokens import Token\n",
    "from json_creator import incarcare_eficienta\n",
    "\n",
    "def synonyms_builder_step1(token: Token, pos_wanted: str)  -> ([str], [str]):\n",
    "    token_text = re.sub('[^a-zA-ZăâîșțĂÂÎȘȚ]', '', token.text.lower())\n",
    "    inflected_forms = all_inflected_forms.find_all_inflected_forms(token_text)\n",
    "    inflection_possibilities = find_inflection_possibilites(token, inflected_forms, pos_wanted)\n",
    "    possible_lexeme_ids = find_lexeme_ids(inflected_forms)\n",
    "    lexeme_ids = find_matching_lexemeIds(possible_lexeme_ids, pos_wanted)\n",
    "    entry_ids = find_entryIds(lexeme_ids)\n",
    "    tree_ids = find_treeIds(entry_ids)\n",
    "    meaning_ids = find_meaningIds(tree_ids)\n",
    "\n",
    "    if len(inflection_possibilities) > 1:\n",
    "        inflection_possibilities =  inflection_filter(token=token, inflection_possibilities=inflection_possibilities)\n",
    "    return tree_ids, inflection_possibilities, meaning_ids\n",
    "\n",
    "def synonyms_builder_step2(meaning_ids, tree_id_forced, token):\n",
    "    candidate_synonyms_base_form = []\n",
    "    token_text = re.sub('[^a-zA-ZăâîșțĂÂÎȘȚ]', '', token.text.lower())\n",
    "    # print(tree_id_forced, \"tree ids forced step2\")\n",
    "\n",
    "    for meaningId in meaning_ids:\n",
    "        \n",
    "        possible_synonyms = synonyms.find_synonyms(meaningId)\n",
    "        tree_ids_verifier = [syn[0] for syn in possible_synonyms]\n",
    "        # print(tree_ids_verifier, f\"tree ids for meaning id {meaningId}\", int(tree_id_forced[0]) in tree_ids_verifier)\n",
    "\n",
    "        # print(possible_synonyms)\n",
    "        if possible_synonyms != [\"no synonyms\"]:\n",
    "        \n",
    "            for synonym in possible_synonyms:\n",
    "                # aici adauga si treeid si mai departe selectezi doar form si cand le sugerezi dai pe cele cu matching treeid (chosencontext de mai jos)\n",
    "                # undo ca inainte stergi syn_treeId\n",
    "                syn_to_add = re.sub('[^a-zA-ZăâîșțĂÂÎȘȚ ]', '', synonym[1]).split(\" \")\n",
    "               \n",
    "                for syn in syn_to_add:\n",
    "                    syn_to_add_helper = all_inflected_forms.find_all_inflected_forms(syn, unidentified={\"lexemeId\": \"UNKNOWN\"})\n",
    "                    if syn_to_add == [\"UNKOWN\"]:\n",
    "                        break\n",
    "\n",
    "                    syn_tuple = (syn, syn_to_add_helper[0].get(\"lexemeId\", \"dummy\"))\n",
    "                    if syn_tuple not in candidate_synonyms_base_form and syn_tuple[0] != token_text:  \n",
    "                        if int(tree_id_forced[0]) in tree_ids_verifier:      \n",
    "                            candidate_synonyms_base_form.append(syn_tuple)\n",
    "\n",
    "    candidate_synonyms_base_form = [syn for i, syn in enumerate(candidate_synonyms_base_form) if i == 0 or syn[1] != candidate_synonyms_base_form[i-1][1]]\n",
    "    # print(candidate_synonyms_base_form, \"!!!!\")\n",
    "    return candidate_synonyms_base_form\n",
    "\n",
    "def is_valid_for_syn(token: Token) -> bool:\n",
    "    if token.pos_ == \"PUNCT\":\n",
    "        return False\n",
    "    if \"aux\" in token.dep_:\n",
    "        return False\n",
    "    if not token.text.isalpha():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_synonyms(token: Token, tree_id_forced = []) -> [str]:\n",
    "    if is_valid_for_syn(token):\n",
    "        pos_found = ud_to_dex[token.pos_]\n",
    "        tree_ids, inflection_possibilites, meaning_ids = synonyms_builder_step1(token, pos_found)\n",
    "\n",
    "        candidate_synonyms_base_form = synonyms_builder_step2(meaning_ids, tree_id_forced, token)\n",
    "\n",
    "        synonyms_found = []\n",
    "        # print(candidate_synonyms_base_form, \"candidate la sinonime\")\n",
    "        for syn in candidate_synonyms_base_form:\n",
    "            inflected_forms_syn = id_to_inflected_forms.find_id_to_inflected_forms(str(syn[1]))\n",
    "\n",
    "            for inflectionId in inflection_possibilites:\n",
    "                inflection = mapare.find_dexonline_pos_detail(str(inflectionId))\n",
    "\n",
    "                for pos_syn in inflected_forms_syn:\n",
    "                    pos_found_on_syn = pos_syn.get(\"pos\")\n",
    "                    form_found_on_syn = pos_syn.get(\"form\")\n",
    "                    if pos_found_on_syn == inflection:\n",
    "                            if form_found_on_syn not in synonyms_found:\n",
    "                                synonyms_found.append(form_found_on_syn)\n",
    "                                \n",
    "       \n",
    "        return synonyms_found\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def force_plural_noun(token):\n",
    "    associated_tokens = token.subtree\n",
    "    for token in associated_tokens:\n",
    "        if token.pos_ in [\"DET\", \"NUM\", \"PRON\"]:\n",
    "            if token.morph.get(\"Number\", [\"dummy\"])[0] == \"Plur\":\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def force_person_and_number_verb(token):\n",
    "    number, person = \"_\", \"_\"\n",
    "    inf = False\n",
    "    subtree = token.subtree\n",
    "    \n",
    "    for t in subtree:\n",
    "        if t == token:\n",
    "            break\n",
    "        if t.dep_ == \"nsubj\":\n",
    "            number = t.morph.get(\"Number\", [\"Sing\"])[0]\n",
    "\n",
    "            if number == \"Plur\":\n",
    "                number = \"plural\"\n",
    "            else:\n",
    "                number = \"singular\"\n",
    "                        \n",
    "            if t.pos_ == \"NOUN\":\n",
    "                person = \"a III-a\"\n",
    "            elif t.pos_ == \"PRON\":\n",
    "                person = t.morph.get(\"Person\", [\"dummy\"])[0]\n",
    "\n",
    "                if person == \"1\":\n",
    "                    person = \"I\"\n",
    "                elif person == \"2\":\n",
    "                    person = \"a II-a\"\n",
    "                elif person == \"3\":\n",
    "                    person = \"a III-a\"\n",
    "        elif t.dep_ == \"mark\":\n",
    "            inf = True\n",
    "    \n",
    "            \n",
    "    return inf, number, person\n",
    "\n",
    "def get_verb_tense(token):\n",
    "    mood = token.morph.get(\"Mood\", [\"Ind\"])[0]\n",
    "    tense = token.morph.get(\"Tense\", [\"dummy\"])[0]\n",
    "    verbform = token.morph.get(\"VerbForm\", [\"dummy\"])[0]\n",
    "\n",
    "    if tense == \"Imp\" or tense == \"Pres\" and mood == \"Indicativ\" and verbform == \"Inf\":\n",
    "        tense = \"imperfect\"\n",
    "    elif tense == \"Pres\":\n",
    "        tense = \"prezent\"\n",
    "    elif tense == \"Past\":\n",
    "        tense = \"perfect simplu\"\n",
    "    elif tense == \"Pqp\":\n",
    "        tense = \"mai mult ca perfect\"\n",
    "    return tense\n",
    "\n",
    "def build_inflection_for_verb(token, inflection_dex_details):\n",
    "    mood = \"Indicativ\" if token.morph.get(\"Mood\", [\"Ind\"])[0] == \"Ind\" else \"None\"\n",
    "\n",
    "    inf, number, person = force_person_and_number_verb(token)\n",
    "\n",
    "    tense = get_verb_tense(token)\n",
    "\n",
    "    if token.dep_ == \"ccomp\" or inf == True:\n",
    "        found_dex_pos = \"Verb, Infinitiv prezent\"\n",
    "    else:\n",
    "        found_dex_pos = f\"Verb, {mood}, {tense}, persoana {person}, {number}\"\n",
    "\n",
    "    if found_dex_pos == inflection_dex_details:\n",
    "        return True\n",
    "    else:\n",
    "        found_dex_pos = f\"Verb, {mood}, prezent, persoana {person}, {number}\"\n",
    "        if found_dex_pos == inflection_dex_details:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_case_for_noun(token):\n",
    "    case = token.morph.get(\"Case\", [\"Acc\", \"Nom\"])\n",
    "    if \"Acc\" in case[0] or \"Nom\" in case[0]:\n",
    "        case = \"Acc, Nom\"\n",
    "\n",
    "    if case == \"Acc, Nom\":\n",
    "        case = \"Nominativ-Acuzativ\"\n",
    "    elif case == \"Dat, Gen\":\n",
    "        case = \"Genitiv-Dativ\"\n",
    "    else:\n",
    "        case = \"Vocativ\"\n",
    "    return case\n",
    "\n",
    "def get_definite(token):\n",
    "    definite = token.morph.get(\"Definite\", [\"dummy\"])[0]\n",
    "    if definite == \"Ind\":\n",
    "        definite = \"nearticulat\"\n",
    "    else:\n",
    "        definite = \"articulat\"\n",
    "    return definite\n",
    "\n",
    "def get_number(token):\n",
    "    number = token.morph.get(\"Number\", [\"Sing\"])[0]\n",
    "    \n",
    "    if force_plural_noun(token):\n",
    "        number = \"plural\"\n",
    "    elif number == \"Sing\":\n",
    "        number = \"singular\"\n",
    "    else:\n",
    "        number = \"plural\"\n",
    "    return number\n",
    "\n",
    "def get_gender(token):\n",
    "    gender = token.morph.get(\"Gender\", [\"dummy\"])[0]\n",
    "    if gender == \"Masc\":\n",
    "        gender = \"masculin\"\n",
    "    else: \n",
    "        gender = \"feminin\"\n",
    "\n",
    "def get_case_for_pron(token):\n",
    "    case = token.morph.get(\"Poss\", [\"No\"])[0]\n",
    "    if case == \"Yes\":\n",
    "        case = \"Genitiv-Dativ\"\n",
    "    else:\n",
    "        case = \"Nominativ-Acuzativ\"\n",
    "    return case\n",
    "\n",
    "def build_inflection_for_noun(token, inflection_dex_details):\n",
    "    case = get_case_for_noun(token)\n",
    "    definite = get_definite(token)\n",
    "    number = get_number(token)\n",
    "    gender = get_gender(token)\n",
    "\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        dex_pos = \"Substantiv\"\n",
    "    elif token.pos_ == \"ADJ\":\n",
    "        dex_pos = \"Adjectiv\"\n",
    "\n",
    "    found_dex_pos = f\"{dex_pos} {gender}, {case}, {number}, {definite}\"\n",
    "    if found_dex_pos == inflection_dex_details:\n",
    "        return True\n",
    "    elif f\"{dex_pos} neutru, {case}, {number}, {definite}\" == inflection_dex_details:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def build_inflection_for_pron(token, inflection_dex_details):\n",
    "    case = get_case_for_pron(token)\n",
    "    number = get_number(token)\n",
    "\n",
    "    for gender in [\"masculin\", \"feminin\"]:  \n",
    "        found_dex_pos = f\"Pronume, {gender}, {case}, {number}\"\n",
    "        if found_dex_pos == inflection_dex_details:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inflection_filter(token, inflection_possibilities):\n",
    "    if '85' in inflection_possibilities:\n",
    "        return '85'\n",
    "\n",
    "    for infl in inflection_possibilities:\n",
    "        inflection_dex_details = mapare.find_dexonline_pos_detail(str(infl))\n",
    "        if token.pos_ in [\"VERB\", \"AUX\"]:  \n",
    "            if build_inflection_for_verb(token, inflection_dex_details) is True:\n",
    "                inflection_possibilities = [infl]\n",
    "\n",
    "\n",
    "        elif token.pos_ in [\"NOUN\", \"ADJ\"]:\n",
    "\n",
    "            if build_inflection_for_noun(token, inflection_dex_details) is True:\n",
    "                inflection_possibilities = [infl]\n",
    "\n",
    "        elif token.pos_ in [\"PRON\", \"DET\"]:\n",
    "            \n",
    "            if build_inflection_for_pron(token, inflection_dex_details) is True:\n",
    "                inflection_possibilities = [infl]\n",
    "\n",
    "    return inflection_possibilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "all_contexts = json.load(open(\"util/context.json\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\", do_lower_case=True)\n",
    "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\")\n",
    "\n",
    "\n",
    "def get_embeddings(text):\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_text)\n",
    "\n",
    "        word_embeddings = outputs.last_hidden_state\n",
    "        averaged_embedding = torch.mean(word_embeddings, dim=0)\n",
    "\n",
    "        return averaged_embedding\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "def calculate_similarity(input, compared_to):\n",
    "    if type(input) != \"<class 'torch.Tensor'>\":\n",
    "        input=torch.tensor(input)\n",
    "   \n",
    "    return cos(input, torch.tensor(compared_to))\n",
    "\n",
    "\n",
    "def transform_with_mean_pooling(numpy_array, target_shape=(384,)):\n",
    "    pooled_array = numpy.reshape(numpy_array, (-1, target_shape[0]))\n",
    "    mean_array = numpy.mean(pooled_array, axis=0) \n",
    "    return mean_array\n",
    "\n",
    "\n",
    "def get_similarity_scores_for_syns(actual_context: list, syn_candidate_context: list, mean: bool, reshapeFlag = True):\n",
    "    if actual_context is None or syn_candidate_context is None:\n",
    "        return 0\n",
    "    meansim = []\n",
    "    \n",
    "    if reshapeFlag == True:\n",
    "        if actual_context[0].shape != (384,):\n",
    "            actual_context = [transform_with_mean_pooling(numpy.array(act_ctx)) for act_ctx in actual_context]\n",
    "\n",
    "    for act_ctx in actual_context:\n",
    "        for syn_ctx in syn_candidate_context:\n",
    "            actual_similarity = calculate_similarity(input=act_ctx, compared_to=syn_ctx)\n",
    "            meansim.append(actual_similarity)\n",
    "\n",
    "    if len(meansim):\n",
    "        if mean is True:\n",
    "            return sum(meansim)/len(meansim)\n",
    "        else:\n",
    "            return max(meansim)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_context_for_each_candidate_syn(token_text, pos_wanted):\n",
    "    pos_wanted = ud_to_dex[pos_wanted]\n",
    "    inflected_forms = all_inflected_forms.find_all_inflected_forms(token_text)\n",
    "    possible_lexeme_ids = find_lexeme_ids(inflected_forms)\n",
    "    lexeme_ids = find_matching_lexemeIds(possible_lexeme_ids, pos_wanted)\n",
    "    entry_ids = find_entryIds(lexeme_ids)\n",
    "    tree_ids = find_treeIds(entry_ids)\n",
    "\n",
    "    contexts_found = {}\n",
    "\n",
    "    for treeId in tree_ids:\n",
    "        contexts_found[treeId] = incarcare_eficienta(treeId)\n",
    "    return contexts_found   \n",
    "\n",
    "\"\"\" \n",
    "    Short demo to show how it actually works. Uncomment and run the main() function.\n",
    "\"\"\"\n",
    "\n",
    "Token.set_extension(\"get_synonyms\", method=get_synonyms, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_word(text):\n",
    "    diacritics = {\n",
    "      \"ă\": \"a\",\n",
    "      \"â\": \"a\",\n",
    "      \"î\": \"i\",\n",
    "      \"ș\": \"s\",\n",
    "      \"ț\": \"t\",\n",
    "      \"č\": \"c\",\n",
    "      \"ş\": \"s\",\n",
    "      \"ž\": \"z\",\n",
    "      \"Ä\": \"A\",\n",
    "      \"Â\": \"A\",\n",
    "      \"Î\": \"I\",\n",
    "      \"Ș\": \"S\",\n",
    "      \"Ț\": \"T\",\n",
    "      \"Č\": \"C\",\n",
    "      \"Ș\": \"S\",\n",
    "      \"Ž\": \"Z\",\n",
    "      \"á\": \"a\",\n",
    "      \"é\": \"e\",\n",
    "      \"í\": \"i\",\n",
    "      \"ó\": \"o\",\n",
    "      \"ú\": \"u\",\n",
    "      \"ű\": \"u\",\n",
    "      \"Á\": \"A\",\n",
    "      \"É\": \"E\",\n",
    "      \"Í\": \"I\",\n",
    "      \"Ó\": \"O\",\n",
    "      \"Ú\": \"U\",\n",
    "      \"Ű\": \"U\",\n",
    "      \"ö\": \"o\",\n",
    "      \"Ö\": \"O\",\n",
    "      \"ü\": \"u\",\n",
    "      \"Ü\": \"U\",\n",
    "    }\n",
    "\n",
    "    for k, v in diacritics.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text.lower()\n",
    "\n",
    "def count_consecutive_vowels(word):\n",
    "    vowels = \"aeiouAEIOU\"\n",
    "    consecutive_vowel_count = 0\n",
    "    total_consecutive_vowels = 0\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            consecutive_vowel_count += 1\n",
    "        else:\n",
    "            total_consecutive_vowels += consecutive_vowel_count\n",
    "            consecutive_vowel_count = 0\n",
    "        \n",
    "    total_consecutive_vowels += consecutive_vowel_count\n",
    "    \n",
    "    return total_consecutive_vowels\n",
    "\n",
    "\n",
    "def aproximate_syllables(word: str):\n",
    "    vowels = \"aeiouăîâe\"\n",
    "    groups = [\"ch\", \"gh\"]\n",
    "    word = raw_word(word).lower()\n",
    "    for group in groups:\n",
    "        if group == \"ch\":\n",
    "            word = word.replace(group, \"C\")\n",
    "        elif group == \"gh\":\n",
    "            word = word.replace(group, \"G\")\n",
    "    \n",
    "    i = 1\n",
    "    syllables = []\n",
    "    last_syllable_index = 0\n",
    "    while i < len(word) - 1:\n",
    "        current_char = word[i]\n",
    "        last_char = word[i-1]\n",
    "        next_char = word[i+1]\n",
    "        if i+2 < len(word):\n",
    "            next2_char = word[i+2]\n",
    "        # RULE1\n",
    "        if current_char not in vowels and last_char in vowels and next_char in vowels:\n",
    "            syllables.append(word[last_syllable_index : i])\n",
    "            last_syllable_index = i\n",
    "        # RULE2\n",
    "        elif current_char not in vowels and next_char not in vowels and last_char in vowels and next2_char in vowels:\n",
    "            # case 1\n",
    "            if current_char in \"bcdfghptv\" and next_char in \"lr\":\n",
    "                syllables.append(word[last_syllable_index : i])\n",
    "            # case 2\n",
    "            else:\n",
    "                syllables.append(word[last_syllable_index : i+1])\n",
    "                i+=1\n",
    "            last_syllable_index = i\n",
    "        # RULE3\n",
    "        elif current_char not in vowels and last_char in vowels:\n",
    "            cons_group = [current_char]\n",
    "            j = i + 1 \n",
    "            while j < len(word):\n",
    "                if word[j] not in vowels:\n",
    "                    cons_group.append(word[j])\n",
    "                else:\n",
    "                    break\n",
    "                j+=1\n",
    "            special_cons_groups = [[\"l\", \"p\", \"t\"], [\"m\", \"p\", \"t\"],  [\"n\", \"c\", \"t\"],  [\"n\", \"c\", \"s\"], [\"n\", \"d\", \"v\"], [\"r\", \"c\", \"t\"], [\"r\", \"t\", \"f\"], [\"s\", \"t\", \"m\"]]\n",
    "            \n",
    "            # case1\n",
    "            if cons_group in special_cons_groups:\n",
    "                syllables.append(word[last_syllable_index:j-1])\n",
    "                last_syllable_index = j-1\n",
    "            # case2\n",
    "            else:    \n",
    "                syllables.append(word[last_syllable_index:i+1])\n",
    "                last_syllable_index = i+1\n",
    "            i=j\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    syllables.append(word[last_syllable_index:])\n",
    "    \n",
    "    # handle hiat, diftong, triftong\n",
    "    for syllable in syllables:\n",
    "        vowels_num = count_consecutive_vowels(syllable)\n",
    "        if 1 < vowels_num <= 3:\n",
    "            double_vowel = True\n",
    "            for i in range(len(syllable) - 1):\n",
    "                \n",
    "                if syllable[i] in vowels and syllable[i+1] != syllable[i]:\n",
    "                    # print(syllable[i])\n",
    "                    double_vowel = False\n",
    "            if syllable == syllables[-1] and syllable[-1] == \"i\" and syllable[i-2] == \"i\":\n",
    "                double_vowel = False\n",
    "\n",
    "            if double_vowel is True:\n",
    "                syllables.append(\"dbl_vowel\")\n",
    "        \n",
    "        elif vowels_num > 3:   \n",
    "            syllables.append(\"4vowel\")\n",
    "        # iae, ieu, oeu, oau, eoau, eoeu\n",
    "        \n",
    "        for long_hiat in [\"iae\", \"ieu\", \"oeu\", \"oau\", \"eoau\", \"eoeu\"]:\n",
    "            if long_hiat in syllable:\n",
    "                syllables.append(\"lng_hi\")\n",
    "\n",
    "            # print(syllable)\n",
    "    return syllables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ud_to_dex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIMP: \u001b[39m\u001b[38;5;124m\"\u001b[39m, t2)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# force meaning id ca filter pentru verb ca uram (aduci sinonimele in functie de chosen_context si fortezi)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m==\u001b[39m cuv:\n\u001b[0;32m--> 105\u001b[0m         pos_found \u001b[38;5;241m=\u001b[39m \u001b[43mud_to_dex\u001b[49m[token\u001b[38;5;241m.\u001b[39mpos_]\n\u001b[1;32m    106\u001b[0m         syns \u001b[38;5;241m=\u001b[39m get_matching_syns(token, actual_context\u001b[38;5;241m=\u001b[39mdoc\u001b[38;5;241m.\u001b[39mtext, pos_found\u001b[38;5;241m=\u001b[39mpos_found)\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m syns:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ud_to_dex' is not defined"
     ]
    }
   ],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def choose_meaning(contexts_found, actual_context):\n",
    "    max_score = -100\n",
    "    key_to_return = \"\"\n",
    "    actual_context = torch.mean(get_embeddings(actual_context), dim=0)\n",
    "\n",
    "    for key in contexts_found:\n",
    "        score = get_similarity_scores_for_syns(actual_context=[actual_context], syn_candidate_context=contexts_found[key], mean=False)\n",
    "        print(score)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            key_to_return = key\n",
    "\n",
    "    return key_to_return\n",
    "\n",
    "\n",
    "def heuristic_comparator(word: str, actual_context: str, token_pos: str, dexonline_order: int, syns_number: int):\n",
    "    # these can be modified, still testing\n",
    "    len_weight = -50\n",
    "    number_of_syllabes_weight = -120\n",
    "    freq_in_ro_lang_weight = 30\n",
    "    similarity_with_actual_context_weight = 1500\n",
    "    dexonline_order_weight = 90\n",
    "\n",
    "    def transform_with_mean_pooling(numpy_array, target_shape=(384,)):\n",
    "        pooled_array = numpy.reshape(numpy_array, (-1, target_shape[0]))\n",
    "        mean_array = numpy.mean(pooled_array, axis=0) \n",
    "        return mean_array\n",
    "\n",
    "    emb1 = [torch.tensor(transform_with_mean_pooling(numpy.array(get_embeddings(actual_context))))]\n",
    "   \n",
    "    sin_context = get_context_for_each_candidate_syn(token_text=word, pos_wanted=token_pos)\n",
    "    \n",
    "    similarity_score = 0\n",
    "    if len(sin_context):\n",
    "        for key in sin_context:\n",
    "            similarity_score = get_similarity_scores_for_syns(actual_context=emb1, syn_candidate_context=sin_context[key], mean=False)\n",
    "    \n",
    "    try:\n",
    "        base_form = get_all_forms(nlp(word)[0])[0].get(\"form\")\n",
    "    except:\n",
    "        base_form = word\n",
    "\n",
    "    word_len = len_weight * len(word)\n",
    "    apx_syllables_number = number_of_syllabes_weight * len(aproximate_syllables(word))\n",
    "    freq = freq_in_ro_lang_weight * (zipf_frequency(base_form, 'ro')) \n",
    "    dexonline_order = dexonline_order_weight * (syns_number - dexonline_order)\n",
    "    \n",
    "    return word_len + apx_syllables_number + freq + similarity_with_actual_context_weight * similarity_score + dexonline_order\n",
    "\n",
    "\n",
    "def get_matching_syns(token, actual_context, pos_found):\n",
    "    tree_ids, inflection_possibilites, meaning_ids = synonyms_builder_step1(token, pos_found)\n",
    "    contexts_found = {}\n",
    "        \n",
    "    for treeId in tree_ids:\n",
    "        contexts_found[treeId] = incarcare_eficienta(treeId)    \n",
    "    print(tree_ids)\n",
    "    try:\n",
    "        if len(contexts_found.keys()) > 1:\n",
    "            chosen_context = str(choose_meaning(contexts_found=contexts_found, actual_context=actual_context))\n",
    "        else:\n",
    "            chosen_context = str(list(contexts_found.keys())[0])\n",
    "        print(chosen_context)\n",
    "        sinonime_posibile_dex = token._.get_synonyms([chosen_context])\n",
    "        \n",
    "        sinonime_to_return = []\n",
    "        for x in sinonime_posibile_dex[:10]:\n",
    "            print(x)\n",
    "\n",
    "        for i in range(len(sinonime_posibile_dex)):\n",
    "            syn = sinonime_posibile_dex[i]\n",
    "            if syn == token.text[1:] or syn == sinonime_posibile_dex[i-1][1:]:\n",
    "                continue\n",
    "            else:\n",
    "                sinonime_to_return.append((syn, heuristic_comparator(syn, actual_context, token.pos_, i, len(sinonime_posibile_dex))))\n",
    "        \n",
    "        return sorted(sinonime_to_return, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    except IndexError:\n",
    "        print(\"no synonyms\")\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import spacy\n",
    "t1 = time.time()\n",
    "\n",
    "# pentru teste\n",
    "cuv = \"pas\"\n",
    "cuv2=\"uram\"\n",
    "# contextele pe care vreau sa l testez\n",
    "actual_context = \"Noi ne deplasam la pas.\"\n",
    "\n",
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "doc = nlp(actual_context)\n",
    "\n",
    "def main():\n",
    "    for token in doc:\n",
    "        if token.text == cuv:\n",
    "            pos_found = ud_to_dex[token.pos_]\n",
    "            syns = get_matching_syns(token, actual_context=doc.text, pos_found=pos_found)\n",
    "            if syns:\n",
    "                for x in syns[:10]:\n",
    "                    if x[0] == cuv:\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(x)\n",
    "                \n",
    "    t2 = time.time() - t1\n",
    "    print(\"TIMP: \", t2)\n",
    "\n",
    "main()\n",
    "\n",
    "# force meaning id ca filter pentru verb ca uram (aduci sinonimele in functie de chosen_context si fortezi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26248, 241918, tensor(0.7635, dtype=torch.float64)), (26248, 26248, tensor(0.6032, dtype=torch.float64)), (26248, 58218, tensor(0.5412, dtype=torch.float64)), (26248, 64581, tensor(0.5079, dtype=torch.float64)), (26248, 73302, tensor(0.4759, dtype=torch.float64)), (26248, 55612, tensor(0.3508, dtype=torch.float64)), (26248, 73347, tensor(0.3470, dtype=torch.float64)), (26248, 64235, tensor(0.3011, dtype=torch.float64)), (26248, 59349, tensor(0.2716, dtype=torch.float64)), (26248, 220204, tensor(0.2604, dtype=torch.float64)), (26248, 34771, tensor(0.2095, dtype=torch.float64)), (26248, 27874, tensor(0.2086, dtype=torch.float64)), (26248, 11667, tensor(0.1959, dtype=torch.float64)), (26248, 47232, tensor(0.0689, dtype=torch.float64)), (26248, 57744, tensor(0.0270, dtype=torch.float64))]\n",
      "[(236008, 236008, tensor(0.3590, dtype=torch.float64)), (236008, 229771, tensor(0.3181, dtype=torch.float64)), (236008, 209382, tensor(0.2590, dtype=torch.float64)), (236008, 62506, tensor(0.2455, dtype=torch.float64)), (236008, 15504, tensor(0.2248, dtype=torch.float64)), (236008, 38853, tensor(0.2201, dtype=torch.float64)), (236008, 221857, tensor(0.2074, dtype=torch.float64)), (236008, 56309, tensor(0.1872, dtype=torch.float64)), (236008, 12655, tensor(0.1763, dtype=torch.float64)), (236008, 57354, tensor(0.1603, dtype=torch.float64)), (236008, 61218, tensor(0.1556, dtype=torch.float64)), (236008, 19331, tensor(0.1525, dtype=torch.float64)), (236008, 27887, tensor(0.1434, dtype=torch.float64)), (236008, 41705, tensor(0.1125, dtype=torch.float64))]\n",
      "[(8422, 8422, tensor(0.2021, dtype=torch.float64)), (8422, 54503, tensor(0.1868, dtype=torch.float64)), (8422, 3526, tensor(0.1754, dtype=torch.float64)), (8422, 60803, tensor(0.1705, dtype=torch.float64)), (8422, 54489, tensor(0.1406, dtype=torch.float64)), (8422, 10558, tensor(0.1398, dtype=torch.float64)), (8422, 16633, tensor(0.1164, dtype=torch.float64)), (8422, 35541, tensor(0.1014, dtype=torch.float64)), (8422, 73253, tensor(0.0844, dtype=torch.float64)), (8422, 22813, tensor(0.0814, dtype=torch.float64)), (8422, 17408, tensor(0.0628, dtype=torch.float64)), (8422, 58507, tensor(0.0388, dtype=torch.float64))]\n",
      "[(73288, 73288, tensor(0.3516, dtype=torch.float64)), (73288, 51338, tensor(0.1852, dtype=torch.float64)), (73288, 20517, tensor(0.1719, dtype=torch.float64)), (73288, 1295, tensor(0.1580, dtype=torch.float64)), (73288, 15214, tensor(0.1568, dtype=torch.float64)), (73288, 282000, tensor(0.1340, dtype=torch.float64)), (73288, 16555, tensor(0.0933, dtype=torch.float64)), (73288, 119830, tensor(0.0774, dtype=torch.float64)), (73288, 102192, tensor(0.0722, dtype=torch.float64)), (73288, 71739, tensor(0.0537, dtype=torch.float64)), (73288, 98921, tensor(0.0346, dtype=torch.float64)), (73288, 106107, tensor(0.0085, dtype=torch.float64))]\n",
      "[(90988, 90988, tensor(0.3234, dtype=torch.float64)), (90988, 5771, tensor(0.3116, dtype=torch.float64)), (90988, 68688, tensor(0.2809, dtype=torch.float64)), (90988, 67593, tensor(0.2527, dtype=torch.float64)), (90988, 85775, tensor(0.2366, dtype=torch.float64)), (90988, 49212, tensor(0.2332, dtype=torch.float64)), (90988, 28913, tensor(0.1561, dtype=torch.float64)), (90988, 9078, tensor(0.1373, dtype=torch.float64)), (90988, 68750, tensor(0.1293, dtype=torch.float64)), (90988, 40592, tensor(0.1181, dtype=torch.float64)), (90988, 45960, tensor(0.1166, dtype=torch.float64)), (90988, 16899, tensor(0.0877, dtype=torch.float64))]\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "#MODEL EVALUATION\n",
    "\n",
    "# random tree id din care \"fur\" doua exemple din lista de contexte = test data\n",
    "# compar test data cu alte 10 id uri + id ul din care l-am creat si ma astept sa fie match pe id\n",
    "import copy\n",
    "import random\n",
    "from json_creator import incarcare_eficienta\n",
    "import math\n",
    "\n",
    "def get_random_treeId(minrange: int, maxrange: int) -> int:\n",
    "    return random.randint(minrange, maxrange)\n",
    "\n",
    "def prepare_treeIds(max_retries):\n",
    "    rand_treeIds = {}\n",
    "    max_elements = 0\n",
    "    for _ in range(10):\n",
    "        for attempt in range(max_retries):\n",
    "            treeId = get_random_treeId(1, 2888812)\n",
    "            try:\n",
    "                # treeIdData = context.find_context(treeId)\n",
    "                treeIdData = incarcare_eficienta(treeId)\n",
    "                if len(treeIdData) > 2:\n",
    "                    rand_treeIds[treeId] = treeIdData\n",
    "                    max_elements += 1\n",
    "                    if max_elements > 14:\n",
    "                        return rand_treeIds\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "    return rand_treeIds\n",
    "\n",
    "\n",
    "def get_random_test_data(rand_treeIds: dict) -> list:\n",
    "    randomTreeId = random.choice(list(rand_treeIds.keys()))\n",
    "    num_elements = min(int(math.sqrt(len(rand_treeIds[randomTreeId]))), len(rand_treeIds[randomTreeId]))\n",
    "\n",
    "    max_idx = len(rand_treeIds[randomTreeId]) - 1  \n",
    "    idx_list = random.sample(range(max_idx + 1), num_elements)\n",
    "\n",
    "    test_data_list = []\n",
    "    for idx in reversed(idx_list):\n",
    "        try:\n",
    "            test_data = rand_treeIds[randomTreeId].pop(idx)\n",
    "            test_data_list.append(test_data)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    return test_data_list, rand_treeIds, randomTreeId\n",
    "\n",
    "def model_evaluation():\n",
    "    rand_treeIds_initial = prepare_treeIds(max_retries=100)\n",
    "    rand_treeIds_param = copy.deepcopy(rand_treeIds_initial)\n",
    "\n",
    "    test_data, rand_treeIds, randomTreeId = get_random_test_data(rand_treeIds_param)\n",
    "\n",
    "    sim_scores = []\n",
    "    \n",
    "    for key in rand_treeIds.keys():\n",
    "        sim_scores.append((randomTreeId, key, get_similarity_scores_for_syns(test_data, rand_treeIds[key], mean=True, reshapeFlag=True)))\n",
    "        \n",
    "        # cu nlp.similarity\n",
    "        # for tst in test_data:\n",
    "        #     for tr in rand_treeIds[key]:\n",
    "                # print(tst, tr, nlp(tst).similarity(nlp(tr)))\n",
    "                # sim_scores.append((randomTreeId, key, nlp(tst).similarity(nlp(tr))))\n",
    "\n",
    "    sim_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "    return sim_scores\n",
    "\n",
    "def count_percentage_equal_to_one(list_data):\n",
    "\n",
    "    count_one = len([element for element in list_data if element == 1])\n",
    "    total_elements = len(list_data)\n",
    "\n",
    "    if total_elements == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (count_one / total_elements) * 100\n",
    "\n",
    "results = []\n",
    "for _ in range(10):\n",
    "    evl = model_evaluation()\n",
    "    score = 1 if evl[0][0] == evl[0][1] or evl[1][0] == evl[1][1] else 0\n",
    "    if score == 1:\n",
    "        print(evl)\n",
    "    results.append(score)\n",
    "\n",
    "print(count_percentage_equal_to_one(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\", do_lower_case=True)\n",
    "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\")\n",
    "# pt fine tuning + antrenare mai departe cred ca trebuie un nou dataset cu label, iar apoi folosit pipelineul de evaluare de mai sus pt modelul tunat\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RO-STS dataset\n",
    "train_file = \"dataset/RO-STS.train.tsv\"\n",
    "dev_file = \"dataset/RO-STS.dev.tsv\"\n",
    "test_file = \"dataset/RO-STS.test.tsv\"\n",
    "\n",
    "def read_tsv_file(filename):\n",
    "  \"\"\"Reads a TSV file and returns a list of dictionaries.\"\"\"\n",
    "  with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    next(f, None)  \n",
    "    data = []\n",
    "    for line in f:\n",
    "      fields = line.strip().split(\"\\t\")\n",
    "      data_point = {\"sentence1\": fields[1], \"sentence2\": fields[2], \"score\": float(fields[0])}\n",
    "      data.append(data_point)\n",
    "  return data\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "dataset[\"train\"] = read_tsv_file(train_file)\n",
    "dataset[\"dev\"] = read_tsv_file(dev_file)\n",
    "dataset[\"test\"] = read_tsv_file(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6837704277788871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l9/4jrhpypx0fq40ckdnkdrqvmh0000gn/T/ipykernel_17218/2386203218.py:17: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(sentence1.similarity(sentence2))\n"
     ]
    }
   ],
   "source": [
    "#EXPERIMENT\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch, util\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\", do_lower_case=True)\n",
    "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\")\n",
    "\n",
    "# schimb aici exemplele si similaritatea cosinus are un sens ok \n",
    "# consider ca principala problema este de la formatul datelor din DEX si ca ar trebui o \"curatare\" a unor caractere: =, nume de scriitori, $, etc\n",
    "sentence1 = nlp(\"El a plecat joi cu prietenii la pescuit.\")\n",
    "sentence2 = nlp(\"Eu voi merge miercuri la pescuit.\")\n",
    "# 220812 context id\n",
    "# 220816 context id\n",
    "# sentence1 = nlp(context.find_context(220812)[0])\n",
    "# sentence2 = nlp(context.find_context(220816)[0])\n",
    "\n",
    "print(sentence1.similarity(sentence2))\n",
    "\n",
    "# print(sentence1, sentence2)\n",
    "# def get_embeddings(text):\n",
    "#         tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**tokenized_text)\n",
    "\n",
    "#         word_embeddings = outputs.last_hidden_state\n",
    "#         averaged_embedding = torch.mean(word_embeddings, dim=0)\n",
    "\n",
    "#         return averaged_embedding\n",
    "\n",
    "# def transform_with_mean_pooling(numpy_array, target_shape=(384,)):\n",
    "#     pooled_array = numpy.reshape(numpy_array, (-1, target_shape[0]))\n",
    "#     mean_array = numpy.mean(pooled_array, axis=0) \n",
    "#     return mean_array\n",
    "\n",
    "# emb1 = torch.tensor(transform_with_mean_pooling(numpy.array(get_embeddings(sentence1))))\n",
    "# emb2 = torch.tensor(transform_with_mean_pooling(numpy.array(get_embeddings(sentence2))))\n",
    "\n",
    "# embeddings1 = torch.mean(get_embeddings(sentence1), dim=0)\n",
    "# embeddings2 = torch.mean(get_embeddings(sentence2), dim=0)\n",
    "\n",
    "# print(embeddings2.shape)\n",
    "# # Calculate cosine similarity\n",
    "# similarity_score = calculate_similarity(embeddings1, embeddings2)\n",
    "# print(f\"Similarity score between sentences: {similarity_score}\")\n",
    "\n",
    "\n",
    "# print(emb1.shape)\n",
    "# similarity_score2 = calculate_similarity(emb1, emb2)\n",
    "# print(f\"Similarity score between sentences: {similarity_score2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul embedding cu modelul\n",
    "# am salvat embedding urile tuturor contextelor in folderul context_stock astfel:\n",
    "#   -> fiecare fisier are 45 de cuvinte ordonate dupa treeId (fiecare cuvant (reprezentat prin treeId) are o lista de numpy arrays care reprezinta embeddingurile exemplelor)\n",
    "#   -> caut eficient => iau tree id si caut binar pe intervalele date din fisierele minrange_maxrange_range.feather\n",
    "#   -> pt calcul de similaritati intre contexte folosesc calculate_similarity\n",
    "\n",
    "# din experimentul de mai sus am vazut ca modelul pare sa calculeze bine similaritatile intre propozitii \"naturale\"\n",
    "# daca dau in schimb doua exemple din contexte din dexonline, se comporta mai ciudat, poate si din cauza formatului lor\n",
    "# exemplele din dexonline mai contin caractere: $, #, nume scrise cu litere mari, \"propozitii\" care nu au predicat\n",
    "# trebuie sa preprocesez? sa antrenez modelul si pe setul de date din dexonline? cum pot face asta pentru ca ar trebui sa stabilesc manual o valoare de cos similarity?\n",
    "\n",
    "# am incercat si alte modele dar rezultatul este acelasi. problema cred ca vine de la reprezentarea embedding urilor\n",
    "\n",
    "\n",
    "# 30% las asa asta e\n",
    "# criterii de organizare a sinonimelor (nu va mai fi similaritate, lungime de caractere, nr de silabe, frecv in a corpus)\n",
    "# cu bert (tokeni din bert - fertility)\n",
    "\n",
    "\n",
    "# SORTARE IN FUNCTIE DE SIMILARITATEA CONTEXTELOR SINONIMELOR\n",
    " # for sin in sinonime_posibile_dex:\n",
    "    #     if sin != token.text:\n",
    "    #         i+=1\n",
    "    #         sin_context = get_context_for_each_candidate_syn(token_text=sin, pos_wanted=token.pos_)\n",
    "\n",
    "    #         for key in sin_context:\n",
    "    #             if key in all_keys_verified:\n",
    "    #                 continue\n",
    "    #             else:\n",
    "    #                 similarity_score = get_similarity_scores_for_syns(actual_context=context_actual_token[chosen_context], syn_candidate_context=sin_context[key], mean=True)\n",
    "    #                 scores.append((similarity_score, key, sin))\n",
    "    #                 all_keys_verified.append(key)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steaua: ['steaua', '4vowel']\n",
      "antiaerian: ['an', 'tiae', 'rian', 'lng_hi']\n",
      "alee: ['a', 'lee', 'dbl_vowel']\n",
      "unghi: ['un', 'Gi']\n",
      "examen: ['e', 'xa', 'men']\n",
      "exercițiu: ['e', 'xer', 'ci', 'tiu']\n",
      "leoaica: ['leoai', 'ca', '4vowel']\n",
      "achitat: ['a', 'Ci', 'tat']\n",
      "leghe: ['le', 'Ge']\n",
      "oglindă: ['o', 'glin', 'da']\n",
      "sculptură: ['sculp', 'tu', 'ra']\n",
      "somptuos: ['somp', 'tuos']\n",
      "funcții: ['func', 'tii']\n",
      "arctic: ['arc', 'tic']\n",
      "jertfă: ['jert', 'fa']\n",
      "astmatic: ['ast', 'ma', 'tic']\n",
      "sculptor: ['sculp', 'tor']\n"
     ]
    }
   ],
   "source": [
    "def raw_word(text):\n",
    "    diacritics = {\n",
    "      \"ă\": \"a\",\n",
    "      \"â\": \"a\",\n",
    "      \"î\": \"i\",\n",
    "      \"ș\": \"s\",\n",
    "      \"ț\": \"t\",\n",
    "      \"č\": \"c\",\n",
    "      \"ş\": \"s\",\n",
    "      \"ž\": \"z\",\n",
    "      \"Ä\": \"A\",\n",
    "      \"Â\": \"A\",\n",
    "      \"Î\": \"I\",\n",
    "      \"Ș\": \"S\",\n",
    "      \"Ț\": \"T\",\n",
    "      \"Č\": \"C\",\n",
    "      \"Ș\": \"S\",\n",
    "      \"Ž\": \"Z\",\n",
    "      \"á\": \"a\",\n",
    "      \"é\": \"e\",\n",
    "      \"í\": \"i\",\n",
    "      \"ó\": \"o\",\n",
    "      \"ú\": \"u\",\n",
    "      \"ű\": \"u\",\n",
    "      \"Á\": \"A\",\n",
    "      \"É\": \"E\",\n",
    "      \"Í\": \"I\",\n",
    "      \"Ó\": \"O\",\n",
    "      \"Ú\": \"U\",\n",
    "      \"Ű\": \"U\",\n",
    "      \"ö\": \"o\",\n",
    "      \"Ö\": \"O\",\n",
    "      \"ü\": \"u\",\n",
    "      \"Ü\": \"U\",\n",
    "    }\n",
    "\n",
    "    for k, v in diacritics.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text.lower()\n",
    "\n",
    "def count_consecutive_vowels(word):\n",
    "    vowels = \"aeiouAEIOU\"\n",
    "    consecutive_vowel_count = 0\n",
    "    total_consecutive_vowels = 0\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            consecutive_vowel_count += 1\n",
    "        else:\n",
    "            total_consecutive_vowels += consecutive_vowel_count\n",
    "            consecutive_vowel_count = 0\n",
    "        \n",
    "    total_consecutive_vowels += consecutive_vowel_count\n",
    "    \n",
    "    return total_consecutive_vowels\n",
    "\n",
    "\n",
    "def split_syllables(word: str):\n",
    "    vowels = \"aeiouăîâe\"\n",
    "    diacritics = \"ăâîșțčşžÄÂÎȘȚČȘŽáéíóúűÁÉÍÓÚŰöÖüÜ\"\n",
    "    groups = [\"ch\", \"gh\"]\n",
    "    word = raw_word(word).lower()\n",
    "    for group in groups:\n",
    "        if group == \"ch\":\n",
    "            word = word.replace(group, \"C\")\n",
    "        elif group == \"gh\":\n",
    "            word = word.replace(group, \"G\")\n",
    "    \n",
    "    i = 1\n",
    "    syllables = []\n",
    "    last_syllable_index = 0\n",
    "    while i < len(word) - 1:\n",
    "        current_char = word[i]\n",
    "        last_char = word[i-1]\n",
    "        next_char = word[i+1]\n",
    "        if i+2 < len(word):\n",
    "            next2_char = word[i+2]\n",
    "        # RULE1\n",
    "        if current_char not in vowels and last_char in vowels and next_char in vowels:\n",
    "            syllables.append(word[last_syllable_index : i])\n",
    "            last_syllable_index = i\n",
    "        # RULE2\n",
    "        elif current_char not in vowels and next_char not in vowels and last_char in vowels and next2_char in vowels:\n",
    "            # case 1\n",
    "            if current_char in \"bcdfghptv\" and next_char in \"lr\":\n",
    "                syllables.append(word[last_syllable_index : i])\n",
    "            # case 2\n",
    "            else:\n",
    "                syllables.append(word[last_syllable_index : i+1])\n",
    "                i+=1\n",
    "            last_syllable_index = i\n",
    "        # RULE3\n",
    "        elif current_char not in vowels and last_char in vowels:\n",
    "            cons_group = [current_char]\n",
    "            j = i + 1 \n",
    "            while j < len(word):\n",
    "                if word[j] not in vowels:\n",
    "                    cons_group.append(word[j])\n",
    "                else:\n",
    "                    break\n",
    "                j+=1\n",
    "            special_cons_groups = [[\"l\", \"p\", \"t\"], [\"m\", \"p\", \"t\"],  [\"n\", \"c\", \"t\"],  [\"n\", \"c\", \"s\"], [\"n\", \"d\", \"v\"], [\"r\", \"c\", \"t\"], [\"r\", \"t\", \"f\"], [\"s\", \"t\", \"m\"]]\n",
    "            \n",
    "            # case1\n",
    "            if cons_group in special_cons_groups:\n",
    "                syllables.append(word[last_syllable_index:j-1])\n",
    "                last_syllable_index = j-1\n",
    "            # case2\n",
    "            else:    \n",
    "                print(\"AICI\")\n",
    "                syllables.append(word[last_syllable_index:i+1])\n",
    "                last_syllable_index = i+1\n",
    "            i=j\n",
    "        # RULE4\n",
    "        \n",
    "        # if len(syllables):\n",
    "            # a mereu vocala restu semivocale\n",
    "            # daca e doar e = vocala restu semivocale etc, fa un ranking si poti doar sa \"aproximezi\"\n",
    "            # print(syllables)\n",
    "            # print(word[last_syllable_index:], \"restul cuv\")\n",
    "\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    syllables.append(word[last_syllable_index:])\n",
    "    \n",
    "    for syllable in syllables:\n",
    "        vowels_num = count_consecutive_vowels(syllable)\n",
    "        if 1 < vowels_num <= 3:\n",
    "            double_vowel = True\n",
    "            for i in range(len(syllable) - 1):\n",
    "                \n",
    "                if syllable[i] in vowels and syllable[i+1] != syllable[i]:\n",
    "                    # print(syllable[i])\n",
    "                    double_vowel = False\n",
    "            if syllable == syllables[-1] and syllable[-1] == \"i\" and syllable[i-2] == \"i\":\n",
    "                double_vowel = False\n",
    "\n",
    "            if double_vowel is True:\n",
    "                syllables.append(\"dbl_vowel\")\n",
    "        \n",
    "        elif vowels_num > 3:   \n",
    "            syllables.append(\"4vowel\")\n",
    "        # iae, ieu, oeu, oau, eoau, eoeu\n",
    "        \n",
    "        for long_hiat in [\"iae\", \"ieu\", \"oeu\", \"oau\", \"eoau\", \"eoeu\"]:\n",
    "            if long_hiat in syllable:\n",
    "                syllables.append(\"lng_hi\")\n",
    "\n",
    "            # print(syllable)\n",
    "    return syllables\n",
    "\n",
    "\n",
    "# Example usage\n",
    "words = [\"steaua\", \"antiaerian\", \"alee\", \"unghi\", \"examen\", \"exercițiu\", \"leoaica\", \"achitat\", \"leghe\", \"oglindă\", \"sculptură\", \"somptuos\", \"funcții\", \"arctic\", \"jertfă\", \"astmatic\", \"sculptor\"]\n",
    "for word in words:\n",
    "    syllables = split_syllables(word)\n",
    "    print(f\"{word}: {syllables}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55\n"
     ]
    }
   ],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "word = nlp(\"băteam\")\n",
    "base_form = get_all_forms(word[0])[0].get(\"form\")\n",
    "print(zipf_frequency(base_form, 'ro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
