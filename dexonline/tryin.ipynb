{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": 39,
>>>>>>> 90f77b9 (started exploring transformers)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gata..\n"
     ]
    }
   ],
   "source": [
    "banned_ent_types = {\"ORGANIZATION\", \"EVENT\", \"GPE\", \"LOC\"}\n",
    "banned_pos = [\"PUNCT\", \"SPACE\"]\n",
    "reflexive_deps = [\"expl:poss\", \"expl:pv\", \"iobj\", \"obj\"]\n",
    "root_forms = [\"ROOT\", \"advcl\", \"acl\", \"cop\", \"conj\", \"parataxis\"]\n",
    "\n",
    "reflexive_short_to_long_form = {\n",
    "    \"mi-\": \"îmi\",\n",
    "    \"ți-\": \"îți\",\n",
    "    \"și-\": \"își\",\n",
    "    \"v-\": \"vă\",\n",
    "    \"s-\": \"se\",\n",
    "    \"ne-\": \"ne\",\n",
    "    \"te-\": \"te\"\n",
    "}\n",
    "\n",
    "ud_to_dex = {\n",
    "        \"VERB\": \"V\",\n",
    "        \"AUX\": \"V\",\n",
    "        \"PART\": \"I\",\n",
    "        \"NOUN\": \"M\",\n",
    "        \"PROPN\": \"SP\",\n",
    "        \"PRON\": \"P\",\n",
    "        \"DET\": \"P\",\n",
    "        \"SCONJ\": \"I\",\n",
    "        \"CCONJ\": \"I\",\n",
    "        \"NUM\": \"P\",\n",
    "        \"INTJ\": \"I\",\n",
    "        \"ADV\": \"I\",\n",
    "        \"ADP\": \"I\",\n",
    "        \"ADJ\": \"A\"\n",
    "   }\n",
    "end_of_phrase = [\"!\", \"?\", \".\", \"\\n\"]\n",
    "\n",
    "json_archive = \"utils_json.zip\"\n",
    "json_archive_url = f\"https://github.com/PetruTH/nlp_lic/releases/download/Resources/{json_archive}\"\n",
    "UNIDENTIFIED_TOKEN = \"unidentified\"\n",
    "MAPARE_PATH = \"util/forme_morfologice.json\"\n",
    "ALL_INFLECTED_FORMS_PATH = \"util/inflected_form_lexemeId_inflectionId.json\"\n",
    "WORD_TO_ID_POS_PATH = \"util/word_id_pos.json\"\n",
    "ID_TO_WORD_POS_PATH = \"util/id_word_pos.json\"\n",
    "ID_TO_INFLECTED_FORMS_PATH = \"util/wordId_inflected_forms.json\"\n",
    "print(\"gata..\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 40,
>>>>>>> 90f77b9 (started exploring transformers)
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      " [INFO] 2024-01-03 14:26:23,030 root:177 - Start loading needed data in memory!\n",
      " [INFO] 2024-01-03 14:26:23,033 data_loader:189 - Mapare file loaded.\n",
      " [INFO] 2024-01-03 14:26:28,568 data_loader:191 - All inflected forms file loaded.\n",
      " [INFO] 2024-01-03 14:26:28,982 data_loader:193 - Mapping word to id and pos file loaded.\n",
      " [INFO] 2024-01-03 14:26:29,382 data_loader:195 - Mapping word id to word and pos file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,241 data_loader:197 - Mapping id to inflected forms file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,445 data_loader:199 - Mapping entry id to lexeme id file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,637 data_loader:201 - Mapping tree id to entry id file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,683 data_loader:203 - Mapping meaning id to tree id file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,811 data_loader:205 - Mapping synonyms file loaded.\n",
      " [INFO] 2024-01-03 14:26:34,185 data_loader:207 - Mapping contexts file loaded.\n",
      " [INFO] 2024-01-03 14:26:34,185 root:209 - The data from jsons files is now loaded in memory!\n"
=======
      " [INFO] 2024-01-05 00:34:48,510 root:177 - Start loading needed data in memory!\n",
      " [INFO] 2024-01-05 00:34:48,513 data_loader:189 - Mapare file loaded.\n",
      " [INFO] 2024-01-05 00:34:58,723 data_loader:191 - All inflected forms file loaded.\n",
      " [INFO] 2024-01-05 00:35:01,100 data_loader:193 - Mapping word to id and pos file loaded.\n",
      " [INFO] 2024-01-05 00:35:01,620 data_loader:195 - Mapping word id to word and pos file loaded.\n",
      " [INFO] 2024-01-05 00:35:05,099 data_loader:197 - Mapping id to inflected forms file loaded.\n",
      " [INFO] 2024-01-05 00:35:05,356 data_loader:199 - Mapping entry id to lexeme id file loaded.\n",
      " [INFO] 2024-01-05 00:35:05,544 data_loader:201 - Mapping tree id to entry id file loaded.\n",
      " [INFO] 2024-01-05 00:35:05,596 data_loader:203 - Mapping meaning id to tree id file loaded.\n",
      " [INFO] 2024-01-05 00:35:05,750 data_loader:205 - Mapping synonyms file loaded.\n",
      " [INFO] 2024-01-05 00:35:06,212 data_loader:207 - Mapping contexts file loaded.\n",
      " [INFO] 2024-01-05 00:35:06,213 root:209 - The data from jsons files is now loaded in memory!\n"
>>>>>>> 90f77b9 (started exploring transformers)
     ]
    }
   ],
   "source": [
    "from data_loader import load_jsons\n",
    "from util_data import (\n",
    "    UNIDENTIFIED_TOKEN,\n",
    "    ud_to_dex,\n",
    "    banned_pos,\n",
    "    banned_ent_types,\n",
    "    reflexive_deps,\n",
    "    reflexive_short_to_long_form\n",
    ")\n",
    "from spacy.tokens import Token\n",
    "\n",
    "mapare, all_inflected_forms, word_to_id_pos, id_to_word_pos, id_to_inflected_forms, entry_lexeme, tree_entry, relation, synonyms, context = load_jsons()\n",
    "\n",
    "\n",
    "def get_all_forms_worker(token: Token) -> [int]:\n",
    "    \"\"\"\n",
    "    thiw will extract every word having inflected form == token.text\n",
    "    \"\"\"\n",
    "    token_text = token.text\n",
    "    if \"-\" in token.text:\n",
    "        token_text = token_text.replace(\"-\", \"\")\n",
    "\n",
    "    all_inflected_words_found = all_inflected_forms.find_all_inflected_forms_double_verification(\n",
    "                token_text, token_text.lower()\n",
    "            )\n",
    "\n",
    "    if all_inflected_words_found == UNIDENTIFIED_TOKEN:\n",
    "        return []\n",
    "\n",
    "    words_prel = []\n",
    "    only_one_word = [word['lexemeId'] for word in all_inflected_words_found]\n",
    "\n",
    "    if len(set(only_one_word)) == 1:\n",
    "        words_prel.append(str(only_one_word[0]))\n",
    "    for word in all_inflected_words_found:\n",
    "        pos_found = mapare.find_dexonline_pos_id(word['inflectionId'])\n",
    "        \"\"\"\n",
    "            mapare.mapping['DEXONLINE_MORPH']: [\"morph dexonline\", \"pos dexonline\"],\n",
    "            this will help for mapping spacy pos to dexonline pos\n",
    "            mapping spacy pos with dexonline pos\n",
    "            looking after an id found from dexonline\n",
    "        \"\"\"\n",
    "\n",
    "        if ud_to_dex[token.pos_] == pos_found:\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "        elif ud_to_dex[token.pos_] == \"M\" and pos_found == \"F\":\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "        elif ud_to_dex[token.pos_] == \"M\" and pos_found == \"N\":\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "    words_prel.sort(key=lambda x: int(x))\n",
    "\n",
    "    return words_prel\n",
    "\n",
    "\n",
    "def get_all_forms(token: Token) -> [{str, str}]:\n",
    "    \"\"\"\n",
    "        This function will return all the inflected forms for a certain token given as a parameter.\n",
    "        It will search for that token in dexonline database and it will find the lexemeId.\n",
    "        Based on get_all_forms_worker, it will choose the word from the list returned that\n",
    "        has lemma like the first form found in dexonline database. After, that,\n",
    "        based on that lexemeId, it will return all inflected forms found with the same lexemeId (a list of\n",
    "        dictionaries containig words form and morphological details also from dexonline database)\n",
    "    \"\"\"\n",
    "    words_prel = get_all_forms_worker(token)\n",
    "    token_text = token.text\n",
    "\n",
    "    if len(words_prel) > 1:\n",
    "        for element in words_prel:\n",
    "            if id_to_word_pos.find_id_to_word_pos_form(element) == token.lemma_:\n",
    "                id = element\n",
    "\n",
    "    elif len(words_prel) == 1:\n",
    "        id = words_prel[0]\n",
    "\n",
    "    elif len(words_prel) == 0:\n",
    "        words_found = word_to_id_pos.find_word_id_pos_double_verification(token.lemma_, token_text)\n",
    "\n",
    "        if words_found != UNIDENTIFIED_TOKEN:\n",
    "            words_prel = [str(x['id']) for x in words_found]\n",
    "            id = words_prel[0]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    result = id_to_inflected_forms.find_id_to_inflected_forms(id)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def validate_token(token: Token) -> bool:\n",
    "    \"\"\"\n",
    "        Function that validates if a token can be found in dexonline database.\n",
    "        It will exclude words that describe names or places, organizations, etc.\n",
    "    \"\"\"\n",
    "    if \"-\" in token.text:\n",
    "        return True\n",
    "    if token.pos_ in banned_pos:\n",
    "        return False\n",
    "    if token.lang_ != \"ro\":\n",
    "        return False\n",
    "    if not token.text.isalpha():\n",
    "        return False\n",
    "    if token.ent_type_ in banned_ent_types:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_wanted_form(token: Token, pos_finder: str, person: str, number: str) -> str:\n",
    "    \"\"\"\n",
    "       This function will return the morph form wanted by pos_finder, person and number\n",
    "    \"\"\"\n",
    "    all_morph = get_all_forms(token)\n",
    "    print(type(all_morph, \"!!!!!\"))\n",
    "    for wanted_form in all_morph:\n",
    "        if pos_finder in wanted_form['pos'] and person in wanted_form['pos'] and number in wanted_form['pos']:\n",
    "            return wanted_form['form']\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def verify_word_at_certain_pos(token: Token, pos_verifier: str) -> bool:\n",
    "    \"\"\"\n",
    "    verifiy if a token is contains a specified string in its part of speech\n",
    "    for example this function will return true if a verb has this description from dexonline\n",
    "    as its pos \"Verb, Indicativ, perfect simplu, persoana I, singular\" and pos_verifier parameter\n",
    "    is \"perfect simplu\" or \"persoana I\", etc\n",
    "    \"\"\"\n",
    "    all_morph = get_all_forms(token)\n",
    "    for wanted_form in all_morph:\n",
    "        if token.text == wanted_form['form']:\n",
    "            for pos in pos_verifier:\n",
    "                if pos in wanted_form['pos']:\n",
    "                    return True\n",
    "\n",
    "\n",
    "def is_composed_subj(token: Token) -> bool:\n",
    "    # extra step to verify if there is a composed subject (like 'eu cu tine mergem')\n",
    "    if not token.pos_ == \"VERB\" and not token.pos_ == \"AUX\":\n",
    "        if len(list(token.children)):\n",
    "            for t in token.children:\n",
    "                if t.text not in [\"m\", \"te\", \"s\"]:\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_right_person_and_number(token: Token) -> (str, str):\n",
    "    \"\"\"\n",
    "        This function will get the person and number data from token.morph\n",
    "        and will convert these into dexonline database format information\n",
    "        in order to select right form of verb.\n",
    "    \"\"\"\n",
    "    # extract correct person and number for a phrase\n",
    "    person = token.morph.get(\"Person\", ['3'])\n",
    "    number = token.morph.get(\"Number\", ['Sing'])\n",
    "\n",
    "    if is_composed_subj(token):\n",
    "        number = [\"Plur\"]\n",
    "\n",
    "    # formatting number and person to be recognized dexonline json\n",
    "    actual_number = \"plural\" if number == [\"Plur\"] else \"singular\"\n",
    "\n",
    "    if person == ['1']:\n",
    "        actual_person = \"I\"\n",
    "    elif person == ['2']:\n",
    "        actual_person = \"II\"\n",
    "    elif person == ['3']:\n",
    "        actual_person = \"III\"\n",
    "\n",
    "    return actual_number, actual_person\n",
    "\n",
    "\n",
    "def forme_reflexive_verifier(token: Token) -> str:\n",
    "    \"\"\"\n",
    "        This function will map short reflexive forms into long ones\n",
    "        using data from reflexive_deps from util_data.py\n",
    "    \"\"\"\n",
    "    word_added = token.text\n",
    "    if token.dep_ in reflexive_deps:\n",
    "        case_condition = token.morph.get(\"Case\", [\"dummy\"])[0] in [\"Dat\", \"Acc\"]\n",
    "        variant_condition = token.morph.get(\"Variant\", [\"dummy\"])[0] == \"Short\"\n",
    "        if case_condition and variant_condition:\n",
    "            word_added = reflexive_short_to_long_form[token.text]\n",
    "\n",
    "    return word_added\n",
    "\n",
    "\n",
    "\n",
    "from spacy.tokens import Token\n",
    "\n",
    "Token.set_extension(\"forms_\", method=get_all_forms, force=True)\n",
    "Token.set_extension(\"is_valid\", method=validate_token, force=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port lista de sinonime dexonline: ['loc'] \n",
      "\n",
      "44381 ['Faptul de a purta sau de a deține.', 'Conduită obișnuită, firească, normală.', 'Îmbrăcăminte caracteristică unui popor, unei regiuni, unei epoci etc.', '$A purta portul$ (cuiva) = a se asemăna, a fi la fel cu cineva, a se adapta la felul de a fi al cuiva.', 'Îmbrăcăminte folosită la anumite ocazii.', '@purta@', '$Portul armelor este interzis.$', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul.$', '$Nu te cunoscusem, fă, mai dinainte! Dar așa ți-e portul?$ COȘBUC, P. I 247.', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul,$ se spune unui fățarnic, care una vorbește și alta face.', 'Aspect, înfățișare a unui lucru.', '$A luat-o pe lîngă casele cu port turcesc.$ GALACTION, O. I 188.', '$Cu furca-n brîu, cu gîndul dus, Era frumoasă de nespus în portu-i de la țară.$ IOSIF, V. 41.', '$S-a-ntins poporul adunat Să joace-n drum după tilinci: Feciori, la zece fete, cinci, Cu zdrîngăneii la opinci, Ca-n port de sat.$ COȘBUC, P. I 57.', '$Cine-a mai dori să facă tovărășie cu tine, aibă-și parte și poarte-ți portul, că pe noi știu că ne-ai amețit.$ CREANGĂ, P. 252.', '$La mijloc stau fecioare și neveste-n largă horă, Toate-n port de sărbătoare.$ COȘBUC, P. I 72.', '$Arald, ce însemnează pe tine negrul port Și fața ta cea albă ca ceara, neschimbată?$ EMINESCU, O. I 97.']\n",
      "0.4500315259484684\n",
      "220857 ['Complex tehnic amenajat pe malul unei ape navigabile, prevăzut cu instalațiile necesare pentru acostarea, adăpostirea și reparația navelor, pentru transportul de mărfuri și de călători.', 'Oraș care are un port[204575].', '@port@', '@porto@', '$În coșuri avea lămîi și portocale, cu care venise desigur din port, de la Dunăre.$ DUMITRIU, N. 104.', '$După cîțiva pași se opriră ca să privească la un vapor care intra în port.$ BART, E. 163.', '$Zgomotul din port pătrunde slab de tot în încăperea scundă.$ DUNĂREANU, N. II.', '$Cînd soarele ajungea la zenit, portul mut, poleit într-o lumină orbitoare, părea în arșița zilei un oraș mort... un oraș fantomă.$ BART, E. 19.', '$Chinuit de dor, de dragostea Axiniei, a plecat în lume s-o întîlnească.... a rătăcit pe mări, prin porturi.$ DUNĂREANU, N. 101.']\n",
      "0.4598337974813249\n",
      "{44381: ['Faptul de a purta sau de a deține.', 'Conduită obișnuită, firească, normală.', 'Îmbrăcăminte caracteristică unui popor, unei regiuni, unei epoci etc.', '$A purta portul$ (cuiva) = a se asemăna, a fi la fel cu cineva, a se adapta la felul de a fi al cuiva.', 'Îmbrăcăminte folosită la anumite ocazii.', '@purta@', '$Portul armelor este interzis.$', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul.$', '$Nu te cunoscusem, fă, mai dinainte! Dar așa ți-e portul?$ COȘBUC, P. I 247.', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul,$ se spune unui fățarnic, care una vorbește și alta face.', 'Aspect, înfățișare a unui lucru.', '$A luat-o pe lîngă casele cu port turcesc.$ GALACTION, O. I 188.', '$Cu furca-n brîu, cu gîndul dus, Era frumoasă de nespus în portu-i de la țară.$ IOSIF, V. 41.', '$S-a-ntins poporul adunat Să joace-n drum după tilinci: Feciori, la zece fete, cinci, Cu zdrîngăneii la opinci, Ca-n port de sat.$ COȘBUC, P. I 57.', '$Cine-a mai dori să facă tovărășie cu tine, aibă-și parte și poarte-ți portul, că pe noi știu că ne-ai amețit.$ CREANGĂ, P. 252.', '$La mijloc stau fecioare și neveste-n largă horă, Toate-n port de sărbătoare.$ COȘBUC, P. I 72.', '$Arald, ce însemnează pe tine negrul port Și fața ta cea albă ca ceara, neschimbată?$ EMINESCU, O. I 97.'], 220857: ['Complex tehnic amenajat pe malul unei ape navigabile, prevăzut cu instalațiile necesare pentru acostarea, adăpostirea și reparația navelor, pentru transportul de mărfuri și de călători.', 'Oraș care are un port[204575].', '@port@', '@porto@', '$În coșuri avea lămîi și portocale, cu care venise desigur din port, de la Dunăre.$ DUMITRIU, N. 104.', '$După cîțiva pași se opriră ca să privească la un vapor care intra în port.$ BART, E. 163.', '$Zgomotul din port pătrunde slab de tot în încăperea scundă.$ DUNĂREANU, N. II.', '$Cînd soarele ajungea la zenit, portul mut, poleit într-o lumină orbitoare, părea în arșița zilei un oraș mort... un oraș fantomă.$ BART, E. 19.', '$Chinuit de dor, de dragostea Axiniei, a plecat în lume s-o întîlnească.... a rătăcit pe mări, prin porturi.$ DUNĂREANU, N. 101.']}\n",
      "TIMP:  3.120910167694092\n"
=======
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/inttstbrd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/inttstbrd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      " [INFO] 2024-01-05 14:40:13,189 sentence_transformers.SentenceTransformer:66 - Load pretrained SentenceTransformer: xlm-r-100langs-bert-base-nli-stsb-mean-tokens\n",
      " [INFO] 2024-01-05 14:40:14,831 sentence_transformers.SentenceTransformer:105 - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bancă lista de sinonime dexonline: [] \n",
      "\n",
      "\n",
      "\n",
      " 5177 ['Scaun, de obicei cu pupitru în față, pentru școlari.', '@banc@', 'Scaun lung și îngust pentru două sau mai multe persoane.', '$Banca ministerială$ = locurile din parlament rezervate membrilor guvernului.', '$Banca acuzaților$ = locurile dintr-o sală de tribunal pe care stau acuzații.', '$Banca apărării$ = locurile dintr-o sală de tribunal destinate avocaților care apără pe acuzați.', '$(De) pe băncile școlii$ = (de) la școală, din (sau în) timpul petrecut în școală.', '$A sta$ (sau $a rămâne$) $în banca sa$ = a rămâne la locul său, a sta pasiv față de orice inițiativă, a fi docil.', '$Pe o bancă de piatră șezuseră pe vremuri arnăuții de pază.$ DUMITRIU, B. F. 118.', '$Așezați pe băncile de lemn ale vagonului, moții priveau munții.$ BOGZA, Ț. 17.', '$Au tras banca de lemn sub fereastră și s-au așezai cu spatele spre lumea din salon.$ PAS, L. I 24.', '$Stăteam amîndoi într-o bancă.$ SADOVEANU, N. F. 36.', '$Capul greu cădea pe bancă, păreau toate-n infinit; Cînd suna, știam că Ramses trebuia să fi murit.$ EMINESCU, O. I 140.', '$S-a remarcat încă de pe băncile școlii.$', 'Aparat de gimnastică, la care se pot executa, individual sau în grup, diferite exerciții de gimnastică.'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITATEA PENTRU Eu am fost la bancă pentru a depune bani. cu 5177 este tensor([[0.6337]])\n",
      "\n",
      "\n",
      " 220622 ['Instituție financiară care are ca activitate principală atragerea de depozite și împrumutarea unor sume în scopul acordării de credite și efectuării de plasamente.', '$Bancă de organe$ = laborator în care se asigură prezervarea în condiții riguroase a unor țesuturi, a sângelui sau a unor organe în vederea transplantării lor.', '(La unele jocuri de cărți) Sumă pe care bancherul[194783] o ține în fața lui pentru a plăti câștigurile celorlalți jucători.', '$A sări$ (sau $a face să sară$) $banca (în aer)$ = a câștiga un pot egal cu întreaga sumă pusă de bancher[194783] în joc.', '@banca@', '@banque@', '$Bancă de stat.$', '$Bancă de date$ sau $bancă de informații$ = totalitatea datelor organizate în scopul optimizării procesului de căutare și modificare a lor sau a relațiilor dintre ele, independent de o anumită aplicație.', '$Banca de credit pentru investiții$ = așezământ care acordă credite pentru investiții în industrie, în comerțul de stat etc.', '$Bancă de emisiune$ = bancă care are dreptul de a emite bancnotă.', '$Bilet[195425] de bancă[195425]$', '$Jucau de vreo două ceasuri bacara... Augustatos... ținea banca.$ DUMITRIU, B. F. 128.'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITATEA PENTRU Eu am fost la bancă pentru a depune bani. cu 220622 este tensor([[0.7757]])\n",
      "\n",
      "\n",
      " 5176 ['@banc@', 'Ridicătură de depuneri de nisip, de pietriș sau de nămol, formată pe fundul mărilor sau în albia apelor curgătoare, care ajunge uneori până la suprafața apei.', 'Grup mare de scoici sau de pești, de obicei de aceeași specie, vârstă și mărime.', '$Un val năpraznic luă barca în spinare și o slobozi ușor... într-un banc de nisip.$ GALACTION, O. I 199.', '$Bancuri uriașe de nisipuri, aduse de fluviu, creșteau din mare ca niște insule la suprafață, astupînd canalul.$ BART, E. 391.', '$Banc de gheață$ = masă uriașă de gheață plutitoare, care se întâlnește în mările polare.'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITATEA PENTRU Eu am fost la bancă pentru a depune bani. cu 5176 este tensor([[0.2938]])\n",
      "{5177: ['Scaun, de obicei cu pupitru în față, pentru școlari.', '@banc@', 'Scaun lung și îngust pentru două sau mai multe persoane.', '$Banca ministerială$ = locurile din parlament rezervate membrilor guvernului.', '$Banca acuzaților$ = locurile dintr-o sală de tribunal pe care stau acuzații.', '$Banca apărării$ = locurile dintr-o sală de tribunal destinate avocaților care apără pe acuzați.', '$(De) pe băncile școlii$ = (de) la școală, din (sau în) timpul petrecut în școală.', '$A sta$ (sau $a rămâne$) $în banca sa$ = a rămâne la locul său, a sta pasiv față de orice inițiativă, a fi docil.', '$Pe o bancă de piatră șezuseră pe vremuri arnăuții de pază.$ DUMITRIU, B. F. 118.', '$Așezați pe băncile de lemn ale vagonului, moții priveau munții.$ BOGZA, Ț. 17.', '$Au tras banca de lemn sub fereastră și s-au așezai cu spatele spre lumea din salon.$ PAS, L. I 24.', '$Stăteam amîndoi într-o bancă.$ SADOVEANU, N. F. 36.', '$Capul greu cădea pe bancă, păreau toate-n infinit; Cînd suna, știam că Ramses trebuia să fi murit.$ EMINESCU, O. I 140.', '$S-a remarcat încă de pe băncile școlii.$', 'Aparat de gimnastică, la care se pot executa, individual sau în grup, diferite exerciții de gimnastică.'], 220622: ['Instituție financiară care are ca activitate principală atragerea de depozite și împrumutarea unor sume în scopul acordării de credite și efectuării de plasamente.', '$Bancă de organe$ = laborator în care se asigură prezervarea în condiții riguroase a unor țesuturi, a sângelui sau a unor organe în vederea transplantării lor.', '(La unele jocuri de cărți) Sumă pe care bancherul[194783] o ține în fața lui pentru a plăti câștigurile celorlalți jucători.', '$A sări$ (sau $a face să sară$) $banca (în aer)$ = a câștiga un pot egal cu întreaga sumă pusă de bancher[194783] în joc.', '@banca@', '@banque@', '$Bancă de stat.$', '$Bancă de date$ sau $bancă de informații$ = totalitatea datelor organizate în scopul optimizării procesului de căutare și modificare a lor sau a relațiilor dintre ele, independent de o anumită aplicație.', '$Banca de credit pentru investiții$ = așezământ care acordă credite pentru investiții în industrie, în comerțul de stat etc.', '$Bancă de emisiune$ = bancă care are dreptul de a emite bancnotă.', '$Bilet[195425] de bancă[195425]$', '$Jucau de vreo două ceasuri bacara... Augustatos... ținea banca.$ DUMITRIU, B. F. 128.'], 5176: ['@banc@', 'Ridicătură de depuneri de nisip, de pietriș sau de nămol, formată pe fundul mărilor sau în albia apelor curgătoare, care ajunge uneori până la suprafața apei.', 'Grup mare de scoici sau de pești, de obicei de aceeași specie, vârstă și mărime.', '$Un val năpraznic luă barca în spinare și o slobozi ușor... într-un banc de nisip.$ GALACTION, O. I 199.', '$Bancuri uriașe de nisipuri, aduse de fluviu, creșteau din mare ca niște insule la suprafață, astupînd canalul.$ BART, E. 391.', '$Banc de gheață$ = masă uriașă de gheață plutitoare, care se întâlnește în mările polare.']}\n",
      "TIMP:  7.445961952209473\n"
>>>>>>> 90f77b9 (started exploring transformers)
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "def find_lexeme_ids(inflected_forms: [str]) -> [str]:\n",
    "    possible_lexeme_ids = []\n",
    "\n",
    "    if inflected_forms != [\"UNKNOWN\"]:\n",
    "        for inflected_form in inflected_forms:\n",
    "            if inflected_form.get(\"lexemeId\") not in possible_lexeme_ids:\n",
    "                possible_lexeme_ids.append(inflected_form.get(\"lexemeId\"))\n",
    "  \n",
    "    \n",
    "    return possible_lexeme_ids\n",
    "\n",
    "def find_inflection_possibilites(token: Token, inflected_forms: [str], pos_wanted: str) -> [str]:\n",
    "    inflection_possibilites = []\n",
    "\n",
    "    if inflected_forms != [\"UNKNOWN\"]:\n",
    "        for inflected_form in inflected_forms:\n",
    "            inflectionId = mapare.find_dexonline_pos_id(inflected_form[\"inflectionId\"])\n",
    "            \n",
    "            inflected_form_id = str(inflected_form[\"inflectionId\"])\n",
    "\n",
    "            if inflectionId == pos_wanted and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif inflectionId in [\"VT\", \"V\"] and pos_wanted in [\"V\", \"VT\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif inflectionId in [\"M\", \"F\", \"N\"] and pos_wanted in [\"M\", \"F\", \"N\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif token.dep_ in [\"ROOT\", \"nmod\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "\n",
    "    return inflection_possibilites\n",
    "\n",
    "def find_matching_lexemeIds(token: Token, possible_lexeme_ids: [str], pos_wanted: str) -> [str]:\n",
    "    lexeme_ids = [] \n",
    "\n",
    "    for lexemeId in possible_lexeme_ids:\n",
    "        variant = id_to_word_pos.find_id_to_word_pos(lexemeId)\n",
    "        if variant['pos'] == pos_wanted:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        elif variant['pos'] in [\"VT\", \"V\"] and pos_wanted in [\"V\", \"VT\"]:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        elif variant['pos'] in [\"M\", \"F\", \"N\"] and pos_wanted in [\"M\", \"F\", \"N\"]:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        # elif token.dep_ in [\"ROOT\", \"nmod\"]:\n",
    "        #     lexeme_ids.append(lexemeId)\n",
    "    return lexeme_ids\n",
    "\n",
    "def find_entryIds(lexeme_ids: str) -> str:\n",
    "    entry_ids = []\n",
    "    for lexemeId in lexeme_ids:\n",
    "        all_entries = entry_lexeme.find_entry_lexeme(lexemeId)\n",
    "        if all_entries != [\"no entry\"]:\n",
    "            for entry in all_entries:\n",
    "                entry_ids.append(entry)\n",
    "\n",
    "    return entry_ids\n",
    "\n",
    "def find_treeIds(entry_ids: str) -> str:\n",
    "    tree_ids = []\n",
    "    for entryId in entry_ids:\n",
    "        tree_entries = tree_entry.find_tree_entry(entryId)\n",
    "        if tree_entries != [\"no entry tree\"]:\n",
    "            for treeId in tree_entries:\n",
    "                tree_ids.append(treeId)\n",
    "    \n",
    "    return tree_ids\n",
    "\n",
    "def find_meaningIds(tree_ids: str) -> str:\n",
    "    meaning_ids = []\n",
    "\n",
    "    for treeId in tree_ids:\n",
    "        all_meaningIds = relation.find_relation(str(treeId))\n",
    "        if all_meaningIds != [\"no relation\"]:\n",
    "            for meaningId in all_meaningIds:\n",
    "                meaning_ids.append(meaningId)\n",
    "\n",
    "    return meaning_ids\n",
    "\n",
    "\n",
    "def synonyms_builder(token: Token, pos_wanted: str)  -> ([str], [str]):\n",
    "    token_text = re.sub('[^a-zA-ZăâîșțĂÂÎȘȚ]', '', token.text.lower())\n",
    "    inflected_forms = all_inflected_forms.find_all_inflected_forms(token_text)\n",
    "    \n",
    "    inflection_possibilities = find_inflection_possibilites(token, inflected_forms, pos_wanted)\n",
    "    possible_lexeme_ids = find_lexeme_ids(inflected_forms)\n",
<<<<<<< HEAD
    "    lexeme_ids = find_matching_lexemeIds(token, possible_lexeme_ids, pos_wanted)\n",
    "    entry_ids = find_entryIds(lexeme_ids)\n",
=======
    "    # print(possible_lexeme_ids, \"possible lexeme ids\")\n",
    "    lexeme_ids = find_matching_lexemeIds(token, possible_lexeme_ids, pos_wanted)\n",
    "    # print(lexeme_ids, \"lexeme ids\")\n",
    "    entry_ids = find_entryIds(lexeme_ids)\n",
    "    # print(entry_ids, \"entry ids§\")\n",
>>>>>>> 90f77b9 (started exploring transformers)
    "    tree_ids = find_treeIds(entry_ids)\n",
    "    meaning_ids = find_meaningIds(tree_ids)\n",
    "\n",
    "    candidate_synonyms_base_form = []\n",
    "    \n",
    "    for meaningId in meaning_ids:\n",
    "        possible_synonyms = synonyms.find_synonyms(meaningId)\n",
    "        if possible_synonyms != [\"no synonyms\"]:\n",
    "            for synonym in possible_synonyms:\n",
    "                syn_to_add = re.sub('[^a-zA-ZăâîșțĂÂÎȘȚ ]', '', synonym[1]).split(\" \")\n",
    "                \n",
    "                for syn in syn_to_add:\n",
    "                    syn_to_add_helper = all_inflected_forms.find_all_inflected_forms(syn, unidentified={\"lexemeId\": \"UNKNOWN\"})\n",
    "                    if syn_to_add == [\"UNKOWN\"]:\n",
    "                        break\n",
    "\n",
    "                    syn_tuple = (syn, syn_to_add_helper[0].get(\"lexemeId\", \"dummy\"))\n",
    "                    if syn_tuple not in candidate_synonyms_base_form and syn_tuple[0] != token_text:\n",
    "                        candidate_synonyms_base_form.append(syn_tuple)\n",
    "\n",
    "    candidate_synonyms_base_form = [syn for i, syn in enumerate(candidate_synonyms_base_form) if i == 0 or syn[1] != candidate_synonyms_base_form[i-1][1]]\n",
    "\n",
    "    return tree_ids, inflection_possibilities, candidate_synonyms_base_form\n",
    "\n",
    "def is_valid_for_syn(token: Token) -> bool:\n",
    "    if token.pos_ == \"PUNCT\":\n",
    "        return False\n",
    "    if \"aux\" in token.dep_:\n",
    "        return False\n",
    "    if not token.text.isalpha():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_synonyms(token: Token) -> [str]:\n",
    "    if is_valid_for_syn(token):\n",
    "        pos_found = ud_to_dex[token.pos_]\n",
    "        tree_ids, inflection_possibilites, candidate_synonyms_base_form = synonyms_builder(token, pos_found)\n",
    "        synonyms_found = []\n",
    "\n",
    "        for syn in candidate_synonyms_base_form:\n",
    "            inflected_forms_syn = id_to_inflected_forms.find_id_to_inflected_forms(str(syn[1]))\n",
    "\n",
    "            for inflectionId in inflection_possibilites:\n",
    "                inflection = mapare.find_dexonline_pos_detail(str(inflectionId))\n",
    "                               \n",
    "                for pos_syn in inflected_forms_syn:\n",
    "                    pos_found_on_syn = pos_syn.get(\"pos\")\n",
    "                    form_found_on_syn = pos_syn.get(\"form\")\n",
    "                    if pos_found_on_syn == inflection:\n",
    "                            if form_found_on_syn not in synonyms_found:\n",
    "                                synonyms_found.append(form_found_on_syn)\n",
    "        \n",
    "        contexts_found = {}\n",
    "        for treeId in tree_ids:\n",
    "            contexts_found[treeId] = context.find_context(treeId)            \n",
    "\n",
    "        return synonyms_found, contexts_found\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    Short demo to show how it actually works. Uncomment and run the main() function.\n",
    "\"\"\"\n",
    "\n",
    "Token.set_extension(\"get_synonyms\", method=get_synonyms, force=True)\n",
    "\n",
    "import time\n",
    "import spacy\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "reader = open(\"/Users/inttstbrd/Desktop/licenta/nlp_lic/text.txt\", \"r\")\n",
    "text = reader.read()\n",
    "\n",
    "# pentru teste\n",
    "cuv = \"port\"\n",
    "# contextele pe care vreau sa l testez\n",
    "context1 = \"Eu locuiesc aproape de port.\"\n",
    "context2 = \"Eu ador un port popular.\"\n",
    "\n",
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "doc = nlp(context2)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "\n",
    "model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def calculate_context_similarity(input_sentence, sentences):\n",
    "    encoded_sentences = [tokenizer(sentence, return_tensors=\"pt\") for sentence in sentences]\n",
    "    input_encoded = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        context_vectors = [model(**sentence)[\"last_hidden_state\"][:, 0, :].squeeze() for sentence in encoded_sentences]\n",
    "        input_vector = model(**input_encoded)[\"last_hidden_state\"][:, 0, :].squeeze()\n",
    "\n",
    "    similarities = [1 - cosine(input_vector.numpy(), context_vector.numpy()) for context_vector in context_vectors]\n",
    "    print(sum(similarities)/len(similarities))\n",
    "\n",
    "    # for idx, sentence in enumerate(sentences):\n",
    "    #         print(f\"Similaritatea pentru exemplul {idx + 1} cu '{input_sentence}': {similarities.pop(0):.4f}\")\n",
    "    #         print(f\"Propoziția corespunzătoare: '{sentence}'\\n\")\n",
    "    \n",
    "\n",
    "def main():\n",
=======
    "# reader = open(\"/Users/inttstbrd/Desktop/licenta/nlp_lic/text.txt\", \"r\")\n",
    "# text = reader.read()\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def keep_letters_and_digits(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('romanian'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = SnowballStemmer(\"romanian\")\n",
    "    tokens = [stemmer.stem(word) for word in tokens]    \n",
    "    return tokens\n",
    "\n",
    "cuv = \"bancă\"\n",
    "# contextele pe care vreau sa l testez\n",
    "context1 = \"Eu am fost la bancă pentru a depune bani.\"\n",
    "\n",
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "doc = nlp(context1)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample, losses\n",
    "\n",
    "def calculate_context_similarity(model, sentence1, sentence2):\n",
    "    emb1 = model.encode(sentence1)\n",
    "    emb2 = model.encode(sentence2)\n",
    "    cos_sim = util.cos_sim(emb1, emb2)\n",
    "    return cos_sim\n",
    "\n",
    "def main():\n",
    "    model = SentenceTransformer('xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
>>>>>>> 90f77b9 (started exploring transformers)
    "    for token in doc:\n",
    "        if token.text == cuv:\n",
    "            syns = token._.get_synonyms()\n",
    "            # asigura te ca le iei pe toate la singural in loc de token.lemma  \n",
    "            if syns:\n",
    "                print(token, \"lista de sinonime dexonline:\", syns[0], \"\\n\")\n",
    "                for ctx in syns[1]:\n",
<<<<<<< HEAD
    "                    print(ctx, syns[1][ctx])\n",
    "\n",
    "                    # compar propozitia cu toate exemplele din contexte diferite si afisez media\n",
    "                    calculate_context_similarity(context1, syns[1][ctx])\n",
    "\n",
=======
    "                    print(\"\\n\\n\", ctx, syns[1][ctx], \"\\n\")\n",
    "\n",
    "                    mean_similarity = []\n",
    "                    for context in syns[1][ctx]:\n",
    "                        if len(context.split()) > 1:\n",
    "                    # compar propozitia cu toate exemplele din contexte diferite si afisez media\n",
    "                            actual_similarity = calculate_context_similarity(\n",
    "                                model,\n",
    "                                \" \".join(keep_letters_and_digits(context1)),\n",
    "                                \" \".join(keep_letters_and_digits(context))\n",
    "                            )\n",
    "                            mean_similarity.append(actual_similarity)\n",
    "                    try:\n",
    "                        if len(mean_similarity) >= 3:\n",
    "                            mean_similarity.sort(reverse=True)\n",
    "\n",
    "                        top3 = mean_similarity[:3]\n",
    "                        media = sum(top3) / 3\n",
    "\n",
    "                        print(f\"SIMILARITATEA PENTRU {context1} cu {ctx} este {media}\")\n",
    "                    except:\n",
    "                        print(\"pass\")\n",
    "                    \n",
>>>>>>> 90f77b9 (started exploring transformers)
    "                print(syns[1])\n",
    "            \n",
    "    t2 = time.time() - t1\n",
    "    print(\"TIMP: \", t2)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
