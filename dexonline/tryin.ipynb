{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gata..\n"
     ]
    }
   ],
   "source": [
    "banned_ent_types = {\"ORGANIZATION\", \"EVENT\", \"GPE\", \"LOC\"}\n",
    "banned_pos = [\"PUNCT\", \"SPACE\"]\n",
    "reflexive_deps = [\"expl:poss\", \"expl:pv\", \"iobj\", \"obj\"]\n",
    "root_forms = [\"ROOT\", \"advcl\", \"acl\", \"cop\", \"conj\", \"parataxis\"]\n",
    "\n",
    "reflexive_short_to_long_form = {\n",
    "    \"mi-\": \"îmi\",\n",
    "    \"ți-\": \"îți\",\n",
    "    \"și-\": \"își\",\n",
    "    \"v-\": \"vă\",\n",
    "    \"s-\": \"se\",\n",
    "    \"ne-\": \"ne\",\n",
    "    \"te-\": \"te\"\n",
    "}\n",
    "\n",
    "ud_to_dex = {\n",
    "        \"VERB\": \"V\",\n",
    "        \"AUX\": \"V\",\n",
    "        \"PART\": \"I\",\n",
    "        \"NOUN\": \"M\",\n",
    "        \"PROPN\": \"SP\",\n",
    "        \"PRON\": \"P\",\n",
    "        \"DET\": \"P\",\n",
    "        \"SCONJ\": \"I\",\n",
    "        \"CCONJ\": \"I\",\n",
    "        \"NUM\": \"P\",\n",
    "        \"INTJ\": \"I\",\n",
    "        \"ADV\": \"I\",\n",
    "        \"ADP\": \"I\",\n",
    "        \"ADJ\": \"A\"\n",
    "   }\n",
    "end_of_phrase = [\"!\", \"?\", \".\", \"\\n\"]\n",
    "\n",
    "json_archive = \"utils_json.zip\"\n",
    "json_archive_url = f\"https://github.com/PetruTH/nlp_lic/releases/download/Resources/{json_archive}\"\n",
    "UNIDENTIFIED_TOKEN = \"unidentified\"\n",
    "MAPARE_PATH = \"util/forme_morfologice.json\"\n",
    "ALL_INFLECTED_FORMS_PATH = \"util/inflected_form_lexemeId_inflectionId.json\"\n",
    "WORD_TO_ID_POS_PATH = \"util/word_id_pos.json\"\n",
    "ID_TO_WORD_POS_PATH = \"util/id_word_pos.json\"\n",
    "ID_TO_INFLECTED_FORMS_PATH = \"util/wordId_inflected_forms.json\"\n",
    "print(\"gata..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [INFO] 2024-01-03 14:26:23,030 root:177 - Start loading needed data in memory!\n",
      " [INFO] 2024-01-03 14:26:23,033 data_loader:189 - Mapare file loaded.\n",
      " [INFO] 2024-01-03 14:26:28,568 data_loader:191 - All inflected forms file loaded.\n",
      " [INFO] 2024-01-03 14:26:28,982 data_loader:193 - Mapping word to id and pos file loaded.\n",
      " [INFO] 2024-01-03 14:26:29,382 data_loader:195 - Mapping word id to word and pos file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,241 data_loader:197 - Mapping id to inflected forms file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,445 data_loader:199 - Mapping entry id to lexeme id file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,637 data_loader:201 - Mapping tree id to entry id file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,683 data_loader:203 - Mapping meaning id to tree id file loaded.\n",
      " [INFO] 2024-01-03 14:26:33,811 data_loader:205 - Mapping synonyms file loaded.\n",
      " [INFO] 2024-01-03 14:26:34,185 data_loader:207 - Mapping contexts file loaded.\n",
      " [INFO] 2024-01-03 14:26:34,185 root:209 - The data from jsons files is now loaded in memory!\n"
     ]
    }
   ],
   "source": [
    "from data_loader import load_jsons\n",
    "from util_data import (\n",
    "    UNIDENTIFIED_TOKEN,\n",
    "    ud_to_dex,\n",
    "    banned_pos,\n",
    "    banned_ent_types,\n",
    "    reflexive_deps,\n",
    "    reflexive_short_to_long_form\n",
    ")\n",
    "from spacy.tokens import Token\n",
    "\n",
    "mapare, all_inflected_forms, word_to_id_pos, id_to_word_pos, id_to_inflected_forms, entry_lexeme, tree_entry, relation, synonyms, context = load_jsons()\n",
    "\n",
    "\n",
    "def get_all_forms_worker(token: Token) -> [int]:\n",
    "    \"\"\"\n",
    "    thiw will extract every word having inflected form == token.text\n",
    "    \"\"\"\n",
    "    token_text = token.text\n",
    "    if \"-\" in token.text:\n",
    "        token_text = token_text.replace(\"-\", \"\")\n",
    "\n",
    "    all_inflected_words_found = all_inflected_forms.find_all_inflected_forms_double_verification(\n",
    "                token_text, token_text.lower()\n",
    "            )\n",
    "\n",
    "    if all_inflected_words_found == UNIDENTIFIED_TOKEN:\n",
    "        return []\n",
    "\n",
    "    words_prel = []\n",
    "    only_one_word = [word['lexemeId'] for word in all_inflected_words_found]\n",
    "\n",
    "    if len(set(only_one_word)) == 1:\n",
    "        words_prel.append(str(only_one_word[0]))\n",
    "    for word in all_inflected_words_found:\n",
    "        pos_found = mapare.find_dexonline_pos_id(word['inflectionId'])\n",
    "        \"\"\"\n",
    "            mapare.mapping['DEXONLINE_MORPH']: [\"morph dexonline\", \"pos dexonline\"],\n",
    "            this will help for mapping spacy pos to dexonline pos\n",
    "            mapping spacy pos with dexonline pos\n",
    "            looking after an id found from dexonline\n",
    "        \"\"\"\n",
    "\n",
    "        if ud_to_dex[token.pos_] == pos_found:\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "        elif ud_to_dex[token.pos_] == \"M\" and pos_found == \"F\":\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "        elif ud_to_dex[token.pos_] == \"M\" and pos_found == \"N\":\n",
    "            if str(word['lexemeId']) not in words_prel:\n",
    "                words_prel.append(str(word['lexemeId']))\n",
    "\n",
    "    words_prel.sort(key=lambda x: int(x))\n",
    "\n",
    "    return words_prel\n",
    "\n",
    "\n",
    "def get_all_forms(token: Token) -> [{str, str}]:\n",
    "    \"\"\"\n",
    "        This function will return all the inflected forms for a certain token given as a parameter.\n",
    "        It will search for that token in dexonline database and it will find the lexemeId.\n",
    "        Based on get_all_forms_worker, it will choose the word from the list returned that\n",
    "        has lemma like the first form found in dexonline database. After, that,\n",
    "        based on that lexemeId, it will return all inflected forms found with the same lexemeId (a list of\n",
    "        dictionaries containig words form and morphological details also from dexonline database)\n",
    "    \"\"\"\n",
    "    words_prel = get_all_forms_worker(token)\n",
    "    token_text = token.text\n",
    "\n",
    "    if len(words_prel) > 1:\n",
    "        for element in words_prel:\n",
    "            if id_to_word_pos.find_id_to_word_pos_form(element) == token.lemma_:\n",
    "                id = element\n",
    "\n",
    "    elif len(words_prel) == 1:\n",
    "        id = words_prel[0]\n",
    "\n",
    "    elif len(words_prel) == 0:\n",
    "        words_found = word_to_id_pos.find_word_id_pos_double_verification(token.lemma_, token_text)\n",
    "\n",
    "        if words_found != UNIDENTIFIED_TOKEN:\n",
    "            words_prel = [str(x['id']) for x in words_found]\n",
    "            id = words_prel[0]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    result = id_to_inflected_forms.find_id_to_inflected_forms(id)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def validate_token(token: Token) -> bool:\n",
    "    \"\"\"\n",
    "        Function that validates if a token can be found in dexonline database.\n",
    "        It will exclude words that describe names or places, organizations, etc.\n",
    "    \"\"\"\n",
    "    if \"-\" in token.text:\n",
    "        return True\n",
    "    if token.pos_ in banned_pos:\n",
    "        return False\n",
    "    if token.lang_ != \"ro\":\n",
    "        return False\n",
    "    if not token.text.isalpha():\n",
    "        return False\n",
    "    if token.ent_type_ in banned_ent_types:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_wanted_form(token: Token, pos_finder: str, person: str, number: str) -> str:\n",
    "    \"\"\"\n",
    "       This function will return the morph form wanted by pos_finder, person and number\n",
    "    \"\"\"\n",
    "    all_morph = get_all_forms(token)\n",
    "    print(type(all_morph, \"!!!!!\"))\n",
    "    for wanted_form in all_morph:\n",
    "        if pos_finder in wanted_form['pos'] and person in wanted_form['pos'] and number in wanted_form['pos']:\n",
    "            return wanted_form['form']\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def verify_word_at_certain_pos(token: Token, pos_verifier: str) -> bool:\n",
    "    \"\"\"\n",
    "    verifiy if a token is contains a specified string in its part of speech\n",
    "    for example this function will return true if a verb has this description from dexonline\n",
    "    as its pos \"Verb, Indicativ, perfect simplu, persoana I, singular\" and pos_verifier parameter\n",
    "    is \"perfect simplu\" or \"persoana I\", etc\n",
    "    \"\"\"\n",
    "    all_morph = get_all_forms(token)\n",
    "    for wanted_form in all_morph:\n",
    "        if token.text == wanted_form['form']:\n",
    "            for pos in pos_verifier:\n",
    "                if pos in wanted_form['pos']:\n",
    "                    return True\n",
    "\n",
    "\n",
    "def is_composed_subj(token: Token) -> bool:\n",
    "    # extra step to verify if there is a composed subject (like 'eu cu tine mergem')\n",
    "    if not token.pos_ == \"VERB\" and not token.pos_ == \"AUX\":\n",
    "        if len(list(token.children)):\n",
    "            for t in token.children:\n",
    "                if t.text not in [\"m\", \"te\", \"s\"]:\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_right_person_and_number(token: Token) -> (str, str):\n",
    "    \"\"\"\n",
    "        This function will get the person and number data from token.morph\n",
    "        and will convert these into dexonline database format information\n",
    "        in order to select right form of verb.\n",
    "    \"\"\"\n",
    "    # extract correct person and number for a phrase\n",
    "    person = token.morph.get(\"Person\", ['3'])\n",
    "    number = token.morph.get(\"Number\", ['Sing'])\n",
    "\n",
    "    if is_composed_subj(token):\n",
    "        number = [\"Plur\"]\n",
    "\n",
    "    # formatting number and person to be recognized dexonline json\n",
    "    actual_number = \"plural\" if number == [\"Plur\"] else \"singular\"\n",
    "\n",
    "    if person == ['1']:\n",
    "        actual_person = \"I\"\n",
    "    elif person == ['2']:\n",
    "        actual_person = \"II\"\n",
    "    elif person == ['3']:\n",
    "        actual_person = \"III\"\n",
    "\n",
    "    return actual_number, actual_person\n",
    "\n",
    "\n",
    "def forme_reflexive_verifier(token: Token) -> str:\n",
    "    \"\"\"\n",
    "        This function will map short reflexive forms into long ones\n",
    "        using data from reflexive_deps from util_data.py\n",
    "    \"\"\"\n",
    "    word_added = token.text\n",
    "    if token.dep_ in reflexive_deps:\n",
    "        case_condition = token.morph.get(\"Case\", [\"dummy\"])[0] in [\"Dat\", \"Acc\"]\n",
    "        variant_condition = token.morph.get(\"Variant\", [\"dummy\"])[0] == \"Short\"\n",
    "        if case_condition and variant_condition:\n",
    "            word_added = reflexive_short_to_long_form[token.text]\n",
    "\n",
    "    return word_added\n",
    "\n",
    "\n",
    "\n",
    "from spacy.tokens import Token\n",
    "\n",
    "Token.set_extension(\"forms_\", method=get_all_forms, force=True)\n",
    "Token.set_extension(\"is_valid\", method=validate_token, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port lista de sinonime dexonline: ['loc'] \n",
      "\n",
      "44381 ['Faptul de a purta sau de a deține.', 'Conduită obișnuită, firească, normală.', 'Îmbrăcăminte caracteristică unui popor, unei regiuni, unei epoci etc.', '$A purta portul$ (cuiva) = a se asemăna, a fi la fel cu cineva, a se adapta la felul de a fi al cuiva.', 'Îmbrăcăminte folosită la anumite ocazii.', '@purta@', '$Portul armelor este interzis.$', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul.$', '$Nu te cunoscusem, fă, mai dinainte! Dar așa ți-e portul?$ COȘBUC, P. I 247.', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul,$ se spune unui fățarnic, care una vorbește și alta face.', 'Aspect, înfățișare a unui lucru.', '$A luat-o pe lîngă casele cu port turcesc.$ GALACTION, O. I 188.', '$Cu furca-n brîu, cu gîndul dus, Era frumoasă de nespus în portu-i de la țară.$ IOSIF, V. 41.', '$S-a-ntins poporul adunat Să joace-n drum după tilinci: Feciori, la zece fete, cinci, Cu zdrîngăneii la opinci, Ca-n port de sat.$ COȘBUC, P. I 57.', '$Cine-a mai dori să facă tovărășie cu tine, aibă-și parte și poarte-ți portul, că pe noi știu că ne-ai amețit.$ CREANGĂ, P. 252.', '$La mijloc stau fecioare și neveste-n largă horă, Toate-n port de sărbătoare.$ COȘBUC, P. I 72.', '$Arald, ce însemnează pe tine negrul port Și fața ta cea albă ca ceara, neschimbată?$ EMINESCU, O. I 97.']\n",
      "0.4500315259484684\n",
      "220857 ['Complex tehnic amenajat pe malul unei ape navigabile, prevăzut cu instalațiile necesare pentru acostarea, adăpostirea și reparația navelor, pentru transportul de mărfuri și de călători.', 'Oraș care are un port[204575].', '@port@', '@porto@', '$În coșuri avea lămîi și portocale, cu care venise desigur din port, de la Dunăre.$ DUMITRIU, N. 104.', '$După cîțiva pași se opriră ca să privească la un vapor care intra în port.$ BART, E. 163.', '$Zgomotul din port pătrunde slab de tot în încăperea scundă.$ DUNĂREANU, N. II.', '$Cînd soarele ajungea la zenit, portul mut, poleit într-o lumină orbitoare, părea în arșița zilei un oraș mort... un oraș fantomă.$ BART, E. 19.', '$Chinuit de dor, de dragostea Axiniei, a plecat în lume s-o întîlnească.... a rătăcit pe mări, prin porturi.$ DUNĂREANU, N. 101.']\n",
      "0.4598337974813249\n",
      "{44381: ['Faptul de a purta sau de a deține.', 'Conduită obișnuită, firească, normală.', 'Îmbrăcăminte caracteristică unui popor, unei regiuni, unei epoci etc.', '$A purta portul$ (cuiva) = a se asemăna, a fi la fel cu cineva, a se adapta la felul de a fi al cuiva.', 'Îmbrăcăminte folosită la anumite ocazii.', '@purta@', '$Portul armelor este interzis.$', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul.$', '$Nu te cunoscusem, fă, mai dinainte! Dar așa ți-e portul?$ COȘBUC, P. I 247.', '$Ori te poartă cum ți-e vorba, ori vorbește cum ți-e portul,$ se spune unui fățarnic, care una vorbește și alta face.', 'Aspect, înfățișare a unui lucru.', '$A luat-o pe lîngă casele cu port turcesc.$ GALACTION, O. I 188.', '$Cu furca-n brîu, cu gîndul dus, Era frumoasă de nespus în portu-i de la țară.$ IOSIF, V. 41.', '$S-a-ntins poporul adunat Să joace-n drum după tilinci: Feciori, la zece fete, cinci, Cu zdrîngăneii la opinci, Ca-n port de sat.$ COȘBUC, P. I 57.', '$Cine-a mai dori să facă tovărășie cu tine, aibă-și parte și poarte-ți portul, că pe noi știu că ne-ai amețit.$ CREANGĂ, P. 252.', '$La mijloc stau fecioare și neveste-n largă horă, Toate-n port de sărbătoare.$ COȘBUC, P. I 72.', '$Arald, ce însemnează pe tine negrul port Și fața ta cea albă ca ceara, neschimbată?$ EMINESCU, O. I 97.'], 220857: ['Complex tehnic amenajat pe malul unei ape navigabile, prevăzut cu instalațiile necesare pentru acostarea, adăpostirea și reparația navelor, pentru transportul de mărfuri și de călători.', 'Oraș care are un port[204575].', '@port@', '@porto@', '$În coșuri avea lămîi și portocale, cu care venise desigur din port, de la Dunăre.$ DUMITRIU, N. 104.', '$După cîțiva pași se opriră ca să privească la un vapor care intra în port.$ BART, E. 163.', '$Zgomotul din port pătrunde slab de tot în încăperea scundă.$ DUNĂREANU, N. II.', '$Cînd soarele ajungea la zenit, portul mut, poleit într-o lumină orbitoare, părea în arșița zilei un oraș mort... un oraș fantomă.$ BART, E. 19.', '$Chinuit de dor, de dragostea Axiniei, a plecat în lume s-o întîlnească.... a rătăcit pe mări, prin porturi.$ DUNĂREANU, N. 101.']}\n",
      "TIMP:  3.120910167694092\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "def find_lexeme_ids(inflected_forms: [str]) -> [str]:\n",
    "    possible_lexeme_ids = []\n",
    "\n",
    "    if inflected_forms != [\"UNKNOWN\"]:\n",
    "        for inflected_form in inflected_forms:\n",
    "            if inflected_form.get(\"lexemeId\") not in possible_lexeme_ids:\n",
    "                possible_lexeme_ids.append(inflected_form.get(\"lexemeId\"))\n",
    "  \n",
    "    \n",
    "    return possible_lexeme_ids\n",
    "\n",
    "def find_inflection_possibilites(token: Token, inflected_forms: [str], pos_wanted: str) -> [str]:\n",
    "    inflection_possibilites = []\n",
    "\n",
    "    if inflected_forms != [\"UNKNOWN\"]:\n",
    "        for inflected_form in inflected_forms:\n",
    "            inflectionId = mapare.find_dexonline_pos_id(inflected_form[\"inflectionId\"])\n",
    "            \n",
    "            inflected_form_id = str(inflected_form[\"inflectionId\"])\n",
    "\n",
    "            if inflectionId == pos_wanted and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif inflectionId in [\"VT\", \"V\"] and pos_wanted in [\"V\", \"VT\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif inflectionId in [\"M\", \"F\", \"N\"] and pos_wanted in [\"M\", \"F\", \"N\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "            elif token.dep_ in [\"ROOT\", \"nmod\"] and inflected_form_id not in inflection_possibilites:\n",
    "                inflection_possibilites.append(str(inflected_form[\"inflectionId\"]))\n",
    "\n",
    "    return inflection_possibilites\n",
    "\n",
    "def find_matching_lexemeIds(token: Token, possible_lexeme_ids: [str], pos_wanted: str) -> [str]:\n",
    "    lexeme_ids = [] \n",
    "\n",
    "    for lexemeId in possible_lexeme_ids:\n",
    "        variant = id_to_word_pos.find_id_to_word_pos(lexemeId)\n",
    "        if variant['pos'] == pos_wanted:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        elif variant['pos'] in [\"VT\", \"V\"] and pos_wanted in [\"V\", \"VT\"]:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        elif variant['pos'] in [\"M\", \"F\", \"N\"] and pos_wanted in [\"M\", \"F\", \"N\"]:\n",
    "            lexeme_ids.append(lexemeId)\n",
    "        # elif token.dep_ in [\"ROOT\", \"nmod\"]:\n",
    "        #     lexeme_ids.append(lexemeId)\n",
    "    return lexeme_ids\n",
    "\n",
    "def find_entryIds(lexeme_ids: str) -> str:\n",
    "    entry_ids = []\n",
    "    for lexemeId in lexeme_ids:\n",
    "        all_entries = entry_lexeme.find_entry_lexeme(lexemeId)\n",
    "        if all_entries != [\"no entry\"]:\n",
    "            for entry in all_entries:\n",
    "                entry_ids.append(entry)\n",
    "\n",
    "    return entry_ids\n",
    "\n",
    "def find_treeIds(entry_ids: str) -> str:\n",
    "    tree_ids = []\n",
    "    for entryId in entry_ids:\n",
    "        tree_entries = tree_entry.find_tree_entry(entryId)\n",
    "        if tree_entries != [\"no entry tree\"]:\n",
    "            for treeId in tree_entries:\n",
    "                tree_ids.append(treeId)\n",
    "    \n",
    "    return tree_ids\n",
    "\n",
    "def find_meaningIds(tree_ids: str) -> str:\n",
    "    meaning_ids = []\n",
    "\n",
    "    for treeId in tree_ids:\n",
    "        all_meaningIds = relation.find_relation(str(treeId))\n",
    "        if all_meaningIds != [\"no relation\"]:\n",
    "            for meaningId in all_meaningIds:\n",
    "                meaning_ids.append(meaningId)\n",
    "\n",
    "    return meaning_ids\n",
    "\n",
    "\n",
    "def synonyms_builder(token: Token, pos_wanted: str)  -> ([str], [str]):\n",
    "    token_text = re.sub('[^a-zA-ZăâîșțĂÂÎȘȚ]', '', token.text.lower())\n",
    "    inflected_forms = all_inflected_forms.find_all_inflected_forms(token_text)\n",
    "    \n",
    "    inflection_possibilities = find_inflection_possibilites(token, inflected_forms, pos_wanted)\n",
    "    possible_lexeme_ids = find_lexeme_ids(inflected_forms)\n",
    "    lexeme_ids = find_matching_lexemeIds(token, possible_lexeme_ids, pos_wanted)\n",
    "    entry_ids = find_entryIds(lexeme_ids)\n",
    "    tree_ids = find_treeIds(entry_ids)\n",
    "    meaning_ids = find_meaningIds(tree_ids)\n",
    "\n",
    "    candidate_synonyms_base_form = []\n",
    "    \n",
    "    for meaningId in meaning_ids:\n",
    "        possible_synonyms = synonyms.find_synonyms(meaningId)\n",
    "        if possible_synonyms != [\"no synonyms\"]:\n",
    "            for synonym in possible_synonyms:\n",
    "                syn_to_add = re.sub('[^a-zA-ZăâîșțĂÂÎȘȚ ]', '', synonym[1]).split(\" \")\n",
    "                \n",
    "                for syn in syn_to_add:\n",
    "                    syn_to_add_helper = all_inflected_forms.find_all_inflected_forms(syn, unidentified={\"lexemeId\": \"UNKNOWN\"})\n",
    "                    if syn_to_add == [\"UNKOWN\"]:\n",
    "                        break\n",
    "\n",
    "                    syn_tuple = (syn, syn_to_add_helper[0].get(\"lexemeId\", \"dummy\"))\n",
    "                    if syn_tuple not in candidate_synonyms_base_form and syn_tuple[0] != token_text:\n",
    "                        candidate_synonyms_base_form.append(syn_tuple)\n",
    "\n",
    "    candidate_synonyms_base_form = [syn for i, syn in enumerate(candidate_synonyms_base_form) if i == 0 or syn[1] != candidate_synonyms_base_form[i-1][1]]\n",
    "\n",
    "    return tree_ids, inflection_possibilities, candidate_synonyms_base_form\n",
    "\n",
    "def is_valid_for_syn(token: Token) -> bool:\n",
    "    if token.pos_ == \"PUNCT\":\n",
    "        return False\n",
    "    if \"aux\" in token.dep_:\n",
    "        return False\n",
    "    if not token.text.isalpha():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_synonyms(token: Token) -> [str]:\n",
    "    if is_valid_for_syn(token):\n",
    "        pos_found = ud_to_dex[token.pos_]\n",
    "        tree_ids, inflection_possibilites, candidate_synonyms_base_form = synonyms_builder(token, pos_found)\n",
    "        synonyms_found = []\n",
    "\n",
    "        for syn in candidate_synonyms_base_form:\n",
    "            inflected_forms_syn = id_to_inflected_forms.find_id_to_inflected_forms(str(syn[1]))\n",
    "\n",
    "            for inflectionId in inflection_possibilites:\n",
    "                inflection = mapare.find_dexonline_pos_detail(str(inflectionId))\n",
    "                               \n",
    "                for pos_syn in inflected_forms_syn:\n",
    "                    pos_found_on_syn = pos_syn.get(\"pos\")\n",
    "                    form_found_on_syn = pos_syn.get(\"form\")\n",
    "                    if pos_found_on_syn == inflection:\n",
    "                            if form_found_on_syn not in synonyms_found:\n",
    "                                synonyms_found.append(form_found_on_syn)\n",
    "        \n",
    "        contexts_found = {}\n",
    "        for treeId in tree_ids:\n",
    "            contexts_found[treeId] = context.find_context(treeId)            \n",
    "\n",
    "        return synonyms_found, contexts_found\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    Short demo to show how it actually works. Uncomment and run the main() function.\n",
    "\"\"\"\n",
    "\n",
    "Token.set_extension(\"get_synonyms\", method=get_synonyms, force=True)\n",
    "\n",
    "import time\n",
    "import spacy\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "reader = open(\"/Users/inttstbrd/Desktop/licenta/nlp_lic/text.txt\", \"r\")\n",
    "text = reader.read()\n",
    "\n",
    "# pentru teste\n",
    "cuv = \"port\"\n",
    "# contextele pe care vreau sa l testez\n",
    "context1 = \"Eu locuiesc aproape de port.\"\n",
    "context2 = \"Eu ador un port popular.\"\n",
    "\n",
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "doc = nlp(context2)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "\n",
    "model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def calculate_context_similarity(input_sentence, sentences):\n",
    "    encoded_sentences = [tokenizer(sentence, return_tensors=\"pt\") for sentence in sentences]\n",
    "    input_encoded = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        context_vectors = [model(**sentence)[\"last_hidden_state\"][:, 0, :].squeeze() for sentence in encoded_sentences]\n",
    "        input_vector = model(**input_encoded)[\"last_hidden_state\"][:, 0, :].squeeze()\n",
    "\n",
    "    similarities = [1 - cosine(input_vector.numpy(), context_vector.numpy()) for context_vector in context_vectors]\n",
    "    print(sum(similarities)/len(similarities))\n",
    "\n",
    "    # for idx, sentence in enumerate(sentences):\n",
    "    #         print(f\"Similaritatea pentru exemplul {idx + 1} cu '{input_sentence}': {similarities.pop(0):.4f}\")\n",
    "    #         print(f\"Propoziția corespunzătoare: '{sentence}'\\n\")\n",
    "    \n",
    "\n",
    "def main():\n",
    "    for token in doc:\n",
    "        if token.text == cuv:\n",
    "            syns = token._.get_synonyms()\n",
    "            # asigura te ca le iei pe toate la singural in loc de token.lemma  \n",
    "            if syns:\n",
    "                print(token, \"lista de sinonime dexonline:\", syns[0], \"\\n\")\n",
    "                for ctx in syns[1]:\n",
    "                    print(ctx, syns[1][ctx])\n",
    "\n",
    "                    # compar propozitia cu toate exemplele din contexte diferite si afisez media\n",
    "                    calculate_context_similarity(context1, syns[1][ctx])\n",
    "\n",
    "                print(syns[1])\n",
    "            \n",
    "    t2 = time.time() - t1\n",
    "    print(\"TIMP: \", t2)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
